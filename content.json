{"meta":{"title":"Huang's Blog","subtitle":"","description":"The blog site of Fan Huang","author":"Fan Huang","url":"http://muyuhuatang.github.io","root":"/"},"pages":[{"title":"About me","date":"2020-10-01T04:13:18.604Z","updated":"2020-10-01T04:13:18.604Z","comments":true,"path":"about/index.html","permalink":"http://muyuhuatang.github.io/about/index.html","excerpt":"","text":"Education:Central South University, Hunan Province, ChinaBachelor’s degree of Engineering - Computer Science and Technology Nanyang Technological University, SingaporeMaster of Science - Information Systems Contact me&#102;&#x68;&#117;&#x61;&#110;&#x67;&#49;&#56;&#49;&#64;&#x67;&#109;&#x61;&#105;&#x6c;&#x2e;&#99;&#111;&#109; / &#x66;&#x68;&#x75;&#97;&#110;&#x67;&#x30;&#48;&#x34;&#x40;&#101;&#46;&#110;&#x74;&#x75;&#x2e;&#x65;&#x64;&#x75;&#x2e;&#x73;&#103; This is my CV (2020.09.24)"},{"title":"categories","date":"2020-09-24T16:10:27.797Z","updated":"2020-09-24T16:10:27.797Z","comments":true,"path":"categories/index.html","permalink":"http://muyuhuatang.github.io/categories/index.html","excerpt":"","text":""},{"title":"schedule","date":"2020-09-24T16:10:21.058Z","updated":"2020-09-24T16:10:21.058Z","comments":true,"path":"schedule/index.html","permalink":"http://muyuhuatang.github.io/schedule/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-09-08T07:42:33.473Z","updated":"2020-09-08T07:42:33.473Z","comments":true,"path":"tags/index.html","permalink":"http://muyuhuatang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Tools","slug":"Tools","date":"2020-10-27T04:12:53.324Z","updated":"2020-11-12T14:14:48.608Z","comments":true,"path":"2020/10/27/Tools/","link":"","permalink":"http://muyuhuatang.github.io/2020/10/27/Tools/","excerpt":"Useful tools and usage tips","text":"Useful tools and usage tips TeamViewer how to totally delete TeamViewer in Mac NTU internal network would block the sign-in and internet process of TV, can try to use VPN to fix this problem. pc shows online but nothing happened, how to fix / or just change internet or check the firewall settings. Keynote youtube keynote freshman guide / PNG cliparts, Design assets, Commercial-Use Photos for free download free elements every month- photo, ppt, video, voice source","categories":[{"name":"Records","slug":"Records","permalink":"http://muyuhuatang.github.io/categories/Records/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://muyuhuatang.github.io/tags/Tools/"}]},{"title":"Association Rule Mining","slug":"Association-Rule-Mining","date":"2020-10-23T02:41:34.399Z","updated":"2020-11-06T06:22:37.981Z","comments":true,"path":"2020/10/23/Association-Rule-Mining/","link":"","permalink":"http://muyuhuatang.github.io/2020/10/23/Association-Rule-Mining/","excerpt":"Detailed AR mining process","text":"Detailed AR mining process Association ProblemWeka Use Java to launch Weka.jar / Youtube example video / Weka-Apriori parameter meaning / Weka Javadoc - Association output runtime in association progress Preparation Learn - University of Waikato. Course Link: Youtube / Youtube / Learn Course Link: Youtube Dataset repository of UCI Small example of running Apriori in Websit javaTpoint Detailed ppt of Association Rule Mining from USTC Complexity Apriori Algorithm steps Step-1: Determine the support of itemsets in the transactional database, and select the minimum support and confidence. Step-2: Take all supports in the transaction with higher support value than the minimum or selected support value. Step-3: Find all the rules of these subsets that have higher confidence value than the threshold or minimum confidence. Step-4: Sort the rules as the decreasing order of lift. Detailed analysis and C++ implementation Apriori in WEKA WEKA use data set in .arff format tips: 1) use csv2arff or arff2csv 2) can also use [arff functions](https://www.youtube.com/watch?v=pecmx-KuUQA&amp;ab_channel=AsteroidGroup) to change xls and csv files directly into arff files There are 12 parameters could be set before running Apriori reference Apriori properties in WEKA.) Original dataset test information Time labor breast-cancer wisconsin-breast-cancer hypothyroid mushroom letter adult attribute number 17 10 10 30 23 16 15 instance number 57 286 699 3772 8124 20000 32561 setting 1 263 302 296 372 343 526 746 setting 2 289 337 314 587 723 2493 4633 brute estimate 16246 81513 199221 1075057 2315421 5700200 9280211 Setting 1: lowerBoundMinSupport = 0.5; classindex = -1; delta = 0.05; minConfidence = 0.9; minRules = 100 Setting 2: lowerBoundMinSupport = 0.1; classindex = -1; delta = 0.01; minConfidence = 0.95; minRules = 200 The reason for disturbance in the plot is that Apriori in WEKA starts with the upper bound support and incrementally decreases support. The algorithm stops when either the specified number of rules are generated, or the lower bound for min is reached. So the abnormal run time is due to the different stop criteria[1]. Additional dataset test informationIn addition, I choose a set of datasets with similar structure, the experimental outcome fits the theoretical prediction very well. Dataset spectrum disorder screening - adolescent spectrum disorder screening - children spectrum disorder screening - adult absent at work SouthGermanCredit mushroom attribute 21 21 21 21 21 22-&gt;21 instance 104 292 704 740 1000 8124 time / setting 1 303 320 319 317 387 409 time / setting 2 307 346 372 472 953 976 Brute force estimation The brute force step can be summed up as follows. Firstly, generate all association rules. Then, determine whether the item sets fit the criteria by checking through all the instances. Set d as the number of attributes and N as the number of instances, then the number of all association rules is [3 ^ d - 2 ^ (d + 1) + 1]. The overall time complexity is [k * {3 ^ D-2 ^ (D + 1) + 1} * N]. In test experiment, it takes 1ms to estimate the brute force time of D = 4, n = 4, so K is 0.005. When d = 10, n = 57, the total time of “labor” dataset spent could be estimated as 16.246s, so as to other datasets. Plot Error analysisReference http://facweb.cs.depaul.edu/mobasher/classes/ect584/WEKA/associate.html#:~:text=The%20upper%20bound%20for%20minimum,to%200.05%20or%205%25). https://www.cnblogs.com/en-heng/p/5719101.html Debug Reference How to avoid form error in hexo display Association-Rule-Mining / Java-Apriori","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Data Mining","slug":"Courses/Data-Mining","permalink":"http://muyuhuatang.github.io/categories/Courses/Data-Mining/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://muyuhuatang.github.io/tags/Data-Mining/"}]},{"title":"Census Income - Data Analysis","slug":"Census Income Dataset","date":"2020-10-08T07:49:42.000Z","updated":"2020-10-14T06:11:47.864Z","comments":true,"path":"2020/10/08/Census Income Dataset/","link":"","permalink":"http://muyuhuatang.github.io/2020/10/08/Census%20Income%20Dataset/","excerpt":"","text":"Data Science, Classification AnalysisData Cleaning, Feature Engineering, Imputation, and Classification.This Notepad has been designed to be run on top of the Jupyter Tensorflow Docker instance found in the link below: https://github.com/jupyter/docker-stacks/tree/master/tensorflow-notebook Checking Number of CPU’s available to Docker containerIdeally, and for this Notebook to run in a reasonable time, your Docker container should have 4 cores or more available. 1!cat /proc/cpuinfo | awk &#x27;/^processor/&#123;print $3&#125;&#x27; | tail -1 5 Import Standard Python Libraries12import io, os, sys, types, time, datetime, math, random, requests, subprocess, tempfilefrom io import StringIO, BytesIO Packages InstallWe’ll now install a few more libraries. This is an easy way to install libraries in a way that are recognised and managed by conda. Do this once and then comment it out for subsequent runs. 12#!conda install --yes -c conda-forge missingno#!conda install --yes -c anaconda requests Packages UpdateThere’s a lot of packages available to us, and most of them were installed when running the dockerfile that created the docker instance. Let’s make sure they are all up to date. Do this once and then comment it out for subsequent runs. 1#!conda update --yes --all Packages ImportThese are all the packages we’ll be using. Importing individual libraries make it easy for us to use them without having to call the parent libraries. 12345678910111213141516171819202122232425262728293031323334353637383940414243# Data Manipulation import numpy as npimport pandas as pd# Visualization import matplotlib.pyplot as pltimport missingnoimport seaborn as snsfrom pandas.plotting import scatter_matrixfrom mpl_toolkits.mplot3d import Axes3D# Feature Selection and Encodingfrom sklearn.feature_selection import RFE, RFECVfrom sklearn.svm import SVRfrom sklearn.decomposition import PCAfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize# Machine learning import sklearn.ensemble as skefrom sklearn import datasets, model_selection, tree, preprocessing, metrics, linear_modelfrom sklearn.svm import LinearSVCfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, SGDClassifierfrom sklearn.tree import DecisionTreeClassifierimport tensorflow as tf# Grid and Random Searchimport scipy.stats as stfrom scipy.stats import randint as sp_randintfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import RandomizedSearchCV# Metricsfrom sklearn.metrics import precision_recall_fscore_support, roc_curve, auc# Managing Warnings import warningswarnings.filterwarnings(&#x27;ignore&#x27;)# Plot the Figures Inline%matplotlib inline Listing Installed PackagesWe could list all installed packages to check whether a package has already been installed. 123456conda_packages_list = BytesIO(subprocess.Popen([&quot;conda&quot;, &quot;list&quot;], stdout=subprocess.PIPE).communicate()[0])conda_packages_list = pd.read_csv(conda_packages_list, names=[&#x27;Package Name&#x27;,&#x27;Version&#x27;,&#x27;Python Version&#x27;,&#x27;Repo&#x27;,&#x27;Other&#x27;], delim_whitespace=True, engine=&#x27;python&#x27;, skiprows=3)conda_packages_list.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Package Name Version Python Version Repo Other 0 _libgcc_mutex 0.1 conda_forge conda-forge NaN 1 _openmp_mutex 4.5 1_llvm conda-forge NaN 2 absl-py 0.10.0 pypi_0 pypi NaN 3 aiohttp 3.6.2 pypi_0 pypi NaN 4 alembic 1.4.3 pyh9f0ad1d_0 conda-forge NaN ObjectiveIn this Jupyter Notepad, we will using the Census Income Dataset to predict whether an individual’s income exceeds $50K/yr based on census data. The dataset can be found here: https://archive.ics.uci.edu/ml/datasets/adult Data Download and LoadingLet’s download the data and save it to a folder in our local directory called ‘dataset’. Download it once, and then comment the code out for subsequent runs. After downloading the data, we load it directly from Disk into a Pandas Dataframe in Memory. Depending on the memory available to the Docker instance, this may be a problem. The data comes separated into the Training and Test datasets. We will join the two for data exploration, and then separate them again before running our algorithms. 123456789101112131415161718# DownloadDATASET = ( &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;, &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;, &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot;,)def download_data(path=&#x27;dataset&#x27;, urls=DATASET): if not os.path.exists(path): os.mkdir(path) for url in urls: response = requests.get(url) name = os.path.basename(url) with open(os.path.join(path, name), &#x27;wb&#x27;) as f: f.write(response.content)#download_data() 123456789101112131415161718192021# Load Training and Test Data Setsheaders = [&#x27;age&#x27;, &#x27;workclass&#x27;, &#x27;fnlwgt&#x27;, &#x27;education&#x27;, &#x27;education-num&#x27;, &#x27;marital-status&#x27;, &#x27;occupation&#x27;, &#x27;relationship&#x27;, &#x27;race&#x27;, &#x27;sex&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;, &#x27;hours-per-week&#x27;, &#x27;native-country&#x27;, &#x27;predclass&#x27;]training_raw = pd.read_csv(&#x27;dataset/adult.data&#x27;, header=None, names=headers, sep=&#x27;,\\s&#x27;, na_values=[&quot;?&quot;], engine=&#x27;python&#x27;)test_raw = pd.read_csv(&#x27;dataset/adult.test&#x27;, header=None, names=headers, sep=&#x27;,\\s&#x27;, na_values=[&quot;?&quot;], engine=&#x27;python&#x27;, skiprows=1) 1234# Join Datasetsdataset_raw = training_raw.append(test_raw)dataset_raw.reset_index(inplace=True)dataset_raw.drop(&#x27;index&#x27;,inplace=True,axis=1) 12345678910# Displaying the size of the Dataframe in Memorydef convert_size(size_bytes): if size_bytes == 0: return &quot;0B&quot; size_name = (&quot;Bytes&quot;, &quot;KB&quot;, &quot;MB&quot;, &quot;GB&quot;, &quot;TB&quot;, &quot;PB&quot;, &quot;EB&quot;, &quot;ZB&quot;, &quot;YB&quot;) i = int(math.floor(math.log(size_bytes, 1024))) p = math.pow(1024, i) s = round(size_bytes / p, 2) return &quot;%s %s&quot; % (s, size_name[i])convert_size(dataset_raw.memory_usage().sum()) &#39;5.59 MB&#39; Data Exploration - UnivariateWhen exploring our dataset and its features, we have many options available to us. We can explore each feature individually, or compare pairs of features, finding the correlation between. Let’s start with some simple Univariate (one feature) analysis. Features can be of multiple types: Nominal: is for mutual exclusive, but not ordered, categories. Ordinal: is one where the order matters but not the difference between values. Interval: is a measurement where the difference between two values is meaningful. Ratio: has all the properties of an interval variable, and also has a clear definition of 0.0. There are multiple ways of manipulating each feature type, but for simplicity, we’ll define only two feature types: Numerical: any feature that contains numeric values. Categorical: any feature that contains categories, or text. 12# Describing all the Numerical Featuresdataset_raw.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; age fnlwgt education-num capital-gain capital-loss hours-per-week count 48842.000000 4.884200e+04 48842.000000 48842.000000 48842.000000 48842.000000 mean 38.643585 1.896641e+05 10.078089 1079.067626 87.502314 40.422382 std 13.710510 1.056040e+05 2.570973 7452.019058 403.004552 12.391444 min 17.000000 1.228500e+04 1.000000 0.000000 0.000000 1.000000 25% 28.000000 1.175505e+05 9.000000 0.000000 0.000000 40.000000 50% 37.000000 1.781445e+05 10.000000 0.000000 0.000000 40.000000 75% 48.000000 2.376420e+05 12.000000 0.000000 0.000000 45.000000 max 90.000000 1.490400e+06 16.000000 99999.000000 4356.000000 99.000000 12# Describing all the Categorical Featuresdataset_raw.describe(include=[&#x27;O&#x27;]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; workclass education marital-status occupation relationship race sex native-country predclass count 46043 48842 48842 46033 48842 48842 48842 47985 48842 unique 8 16 7 14 6 5 2 41 4 top Private HS-grad Married-civ-spouse Prof-specialty Husband White Male United-States &lt;=50K freq 33906 15784 22379 6172 19716 41762 32650 43832 24720 12# Let&#x27;s have a quick look at our datadataset_raw.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country predclass 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States &lt;=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States &lt;=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States &lt;=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States &lt;=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba &lt;=50K 12345678910111213141516171819# Let’s plot the distribution of each featuredef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5): plt.style.use(&#x27;seaborn-whitegrid&#x27;) fig = plt.figure(figsize=(width,height)) fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace) rows = math.ceil(float(dataset.shape[1]) / cols) for i, column in enumerate(dataset.columns): ax = fig.add_subplot(rows, cols, i + 1) ax.set_title(column) if dataset.dtypes[column] == np.object: g = sns.countplot(y=column, data=dataset) substrings = [s.get_text()[:18] for s in g.get_yticklabels()] g.set(yticklabels=substrings) plt.xticks(rotation=25) else: g = sns.distplot(dataset[column]) plt.xticks(rotation=25) plot_distribution(dataset_raw, cols=3, width=20, height=20, hspace=0.45, wspace=0.5) 12# How many missing values are there in our dataset?missingno.matrix(dataset_raw, figsize = (30,5)) &lt;AxesSubplot:&gt; 1missingno.bar(dataset_raw, sort=&#x27;ascending&#x27;, figsize = (30,5)) &lt;AxesSubplot:&gt; Feature Cleaning, Engineering, and ImputationCleaning:To clean our data, we’ll need to work with: Missing values: Either omit elements from a dataset that contain missing values or impute them (fill them in). Special values: Numeric variables are endowed with several formalized special values including ±Inf, NA and NaN. Calculations involving special values often result in special values, and need to be handled/cleaned. Outliers: They should be detected, but not necessarily removed. Their inclusion in the analysis is a statistical decision. Obvious inconsistencies: A person’s age cannot be negative, a man cannot be pregnant and an under-aged person cannot possess a drivers license. Find the inconsistencies and plan for them. Engineering:There are multiple techniques for feature engineering: Decompose: Converting 2014-09-20T20:45:40Z into categorical attributes like hour_of_the_day, part_of_day, etc. Discretization: We can choose to either discretize some of the continuous variables we have, as some algorithms will perform faster. We are going to do both, and compare the results of the ML algorithms on both discretized and non discretised datasets. We’ll call these datasets: dataset_bin =&gt; where Continuous variables are Discretised dataset_con =&gt; where Continuous variables are Continuous Reframe Numerical Quantities: Changing from grams to kg, and losing detail might be both wanted and efficient for calculation Feature Crossing: Creating new features as a combination of existing features. Could be multiplying numerical features, or combining categorical variables. This is a great way to add domain expertise knowledge to the dataset. Imputation:We can impute missing values in a number of different ways: Hot-Deck: The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. Cold-Deck: Selects donors from another dataset to complete missing data. Mean-substitution: Another imputation technique involves replacing any missing value with the mean of that variable for all other cases, which has the benefit of not changing the sample mean for that variable. Regression: A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing. 123# To perform our data analysis, let&#x27;s create new dataframes.dataset_bin = pd.DataFrame() # To contain our dataframe with our discretised continuous variables dataset_con = pd.DataFrame() # To contain our dataframe with our continuous variables Feature PredclassThis is the feature we are trying to predict. We’ll change the string to a binary 0/1. With 1 signifying over $50K. 12345678# Let&#x27;s fix the Class Featuredataset_raw.loc[dataset_raw[&#x27;predclass&#x27;] == &#x27;&gt;50K&#x27;, &#x27;predclass&#x27;] = 1dataset_raw.loc[dataset_raw[&#x27;predclass&#x27;] == &#x27;&gt;50K.&#x27;, &#x27;predclass&#x27;] = 1dataset_raw.loc[dataset_raw[&#x27;predclass&#x27;] == &#x27;&lt;=50K&#x27;, &#x27;predclass&#x27;] = 0dataset_raw.loc[dataset_raw[&#x27;predclass&#x27;] == &#x27;&lt;=50K.&#x27;, &#x27;predclass&#x27;] = 0dataset_bin[&#x27;predclass&#x27;] = dataset_raw[&#x27;predclass&#x27;]dataset_con[&#x27;predclass&#x27;] = dataset_raw[&#x27;predclass&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,1)) sns.countplot(y=&quot;predclass&quot;, data=dataset_bin); Feature: AgeWe will use the Pandas Cut function to bin the data in equally sized buckets. We will also add our original feature to the dataset_con dataframe. 12dataset_bin[&#x27;age&#x27;] = pd.cut(dataset_raw[&#x27;age&#x27;], 10) # discretised dataset_con[&#x27;age&#x27;] = dataset_raw[&#x27;age&#x27;] # non-discretised 1234567plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,5)) plt.subplot(1, 2, 1)sns.countplot(y=&quot;age&quot;, data=dataset_bin);plt.subplot(1, 2, 2)sns.distplot(dataset_con.loc[dataset_con[&#x27;predclass&#x27;] == 1][&#x27;age&#x27;], kde_kws=&#123;&quot;label&quot;: &quot;&gt;$50K&quot;&#125;);sns.distplot(dataset_con.loc[dataset_con[&#x27;predclass&#x27;] == 0][&#x27;age&#x27;], kde_kws=&#123;&quot;label&quot;: &quot;&lt;$50K&quot;&#125;); Feature: Workclass1234# Can we bucket some of these groups?plt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(20,3)) sns.countplot(y=&quot;workclass&quot;, data=dataset_raw); 123456789101112# There are too many groups here, we can group someof them together.# Create buckets for Workclassdataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Without-pay&#x27;, &#x27;workclass&#x27;] = &#x27;Not Working&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Never-worked&#x27;, &#x27;workclass&#x27;] = &#x27;Not Working&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Federal-gov&#x27;, &#x27;workclass&#x27;] = &#x27;Fed-gov&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;State-gov&#x27;, &#x27;workclass&#x27;] = &#x27;Non-fed-gov&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Local-gov&#x27;, &#x27;workclass&#x27;] = &#x27;Non-fed-gov&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Self-emp-not-inc&#x27;, &#x27;workclass&#x27;] = &#x27;Self-emp&#x27;dataset_raw.loc[dataset_raw[&#x27;workclass&#x27;] == &#x27;Self-emp-inc&#x27;, &#x27;workclass&#x27;] = &#x27;Self-emp&#x27;dataset_bin[&#x27;workclass&#x27;] = dataset_raw[&#x27;workclass&#x27;]dataset_con[&#x27;workclass&#x27;] = dataset_raw[&#x27;workclass&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,2)) sns.countplot(y=&quot;workclass&quot;, data=dataset_bin); Feature: Occupation1234# Can we bucket some of these groups?plt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(20,5)) sns.countplot(y=&quot;occupation&quot;, data=dataset_raw); 123456789101112131415161718# Create buckets for Occupationdataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Adm-clerical&#x27;, &#x27;occupation&#x27;] = &#x27;Admin&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Armed-Forces&#x27;, &#x27;occupation&#x27;] = &#x27;Military&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Craft-repair&#x27;, &#x27;occupation&#x27;] = &#x27;Manual Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Exec-managerial&#x27;, &#x27;occupation&#x27;] = &#x27;Office Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Farming-fishing&#x27;, &#x27;occupation&#x27;] = &#x27;Manual Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Handlers-cleaners&#x27;, &#x27;occupation&#x27;] = &#x27;Manual Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Machine-op-inspct&#x27;, &#x27;occupation&#x27;] = &#x27;Manual Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Other-service&#x27;, &#x27;occupation&#x27;] = &#x27;Service&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Priv-house-serv&#x27;, &#x27;occupation&#x27;] = &#x27;Service&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Prof-specialty&#x27;, &#x27;occupation&#x27;] = &#x27;Professional&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Protective-serv&#x27;, &#x27;occupation&#x27;] = &#x27;Military&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Sales&#x27;, &#x27;occupation&#x27;] = &#x27;Office Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Tech-support&#x27;, &#x27;occupation&#x27;] = &#x27;Office Labour&#x27;dataset_raw.loc[dataset_raw[&#x27;occupation&#x27;] == &#x27;Transport-moving&#x27;, &#x27;occupation&#x27;] = &#x27;Manual Labour&#x27;dataset_bin[&#x27;occupation&#x27;] = dataset_raw[&#x27;occupation&#x27;]dataset_con[&#x27;occupation&#x27;] = dataset_raw[&#x27;occupation&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,3))sns.countplot(y=&quot;occupation&quot;, data=dataset_bin); Feature: Native Country1234# Can we bucket some of these groups?plt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(20,10)) sns.countplot(y=&quot;native-country&quot;, data=dataset_raw); 1234567891011121314151617181920212223242526272829303132333435363738394041424344dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Cambodia&#x27; , &#x27;native-country&#x27;] = &#x27;SE-Asia&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Canada&#x27; , &#x27;native-country&#x27;] = &#x27;British-Commonwealth&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;China&#x27; , &#x27;native-country&#x27;] = &#x27;China&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Columbia&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Cuba&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Dominican-Republic&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Ecuador&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;El-Salvador&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27; dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;England&#x27; , &#x27;native-country&#x27;] = &#x27;British-Commonwealth&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;France&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_1&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Germany&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_1&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Greece&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Guatemala&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Haiti&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Holand-Netherlands&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_1&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Honduras&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Hong&#x27; , &#x27;native-country&#x27;] = &#x27;China&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Hungary&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;India&#x27; , &#x27;native-country&#x27;] = &#x27;British-Commonwealth&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Iran&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Ireland&#x27; , &#x27;native-country&#x27;] = &#x27;British-Commonwealth&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Italy&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_1&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Jamaica&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Japan&#x27; , &#x27;native-country&#x27;] = &#x27;APAC&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Laos&#x27; , &#x27;native-country&#x27;] = &#x27;SE-Asia&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Mexico&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Nicaragua&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Outlying-US(Guam-USVI-etc)&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Peru&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Philippines&#x27; , &#x27;native-country&#x27;] = &#x27;SE-Asia&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Poland&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Portugal&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Puerto-Rico&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Scotland&#x27; , &#x27;native-country&#x27;] = &#x27;British-Commonwealth&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;South&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Taiwan&#x27; , &#x27;native-country&#x27;] = &#x27;China&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Thailand&#x27; , &#x27;native-country&#x27;] = &#x27;SE-Asia&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Trinadad&amp;Tobago&#x27; , &#x27;native-country&#x27;] = &#x27;South-America&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;United-States&#x27; , &#x27;native-country&#x27;] = &#x27;United-States&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Vietnam&#x27; , &#x27;native-country&#x27;] = &#x27;SE-Asia&#x27;dataset_raw.loc[dataset_raw[&#x27;native-country&#x27;] == &#x27;Yugoslavia&#x27; , &#x27;native-country&#x27;] = &#x27;Euro_Group_2&#x27;dataset_bin[&#x27;native-country&#x27;] = dataset_raw[&#x27;native-country&#x27;]dataset_con[&#x27;native-country&#x27;] = dataset_raw[&#x27;native-country&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,4)) sns.countplot(y=&quot;native-country&quot;, data=dataset_bin); Feature: Education1234# Can we bucket some of these groups?plt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(20,5)) sns.countplot(y=&quot;education&quot;, data=dataset_raw); 12345678910111213141516171819dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;10th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;11th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;12th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;1st-4th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;5th-6th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;7th-8th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;9th&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Assoc-acdm&#x27; , &#x27;education&#x27;] = &#x27;Associate&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Assoc-voc&#x27; , &#x27;education&#x27;] = &#x27;Associate&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Bachelors&#x27; , &#x27;education&#x27;] = &#x27;Bachelors&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Doctorate&#x27; , &#x27;education&#x27;] = &#x27;Doctorate&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;HS-Grad&#x27; , &#x27;education&#x27;] = &#x27;HS-Graduate&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Masters&#x27; , &#x27;education&#x27;] = &#x27;Masters&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Preschool&#x27; , &#x27;education&#x27;] = &#x27;Dropout&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Prof-school&#x27; , &#x27;education&#x27;] = &#x27;Professor&#x27;dataset_raw.loc[dataset_raw[&#x27;education&#x27;] == &#x27;Some-college&#x27; , &#x27;education&#x27;] = &#x27;HS-Graduate&#x27;dataset_bin[&#x27;education&#x27;] = dataset_raw[&#x27;education&#x27;]dataset_con[&#x27;education&#x27;] = dataset_raw[&#x27;education&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,4)) sns.countplot(y=&quot;education&quot;, data=dataset_bin); Feature: Marital Status123# Can we bucket some of these groups?plt.figure(figsize=(20,3)) sns.countplot(y=&quot;marital-status&quot;, data=dataset_raw); 12345678910dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Never-married&#x27; , &#x27;marital-status&#x27;] = &#x27;Never-Married&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Married-AF-spouse&#x27; , &#x27;marital-status&#x27;] = &#x27;Married&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Married-civ-spouse&#x27; , &#x27;marital-status&#x27;] = &#x27;Married&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Married-spouse-absent&#x27;, &#x27;marital-status&#x27;] = &#x27;Not-Married&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Separated&#x27; , &#x27;marital-status&#x27;] = &#x27;Separated&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Divorced&#x27; , &#x27;marital-status&#x27;] = &#x27;Separated&#x27;dataset_raw.loc[dataset_raw[&#x27;marital-status&#x27;] == &#x27;Widowed&#x27; , &#x27;marital-status&#x27;] = &#x27;Widowed&#x27;dataset_bin[&#x27;marital-status&#x27;] = dataset_raw[&#x27;marital-status&#x27;]dataset_con[&#x27;marital-status&#x27;] = dataset_raw[&#x27;marital-status&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,3)) sns.countplot(y=&quot;marital-status&quot;, data=dataset_bin); Feature: Final Weight123# Let&#x27;s use the Pandas Cut function to bin the data in equally sized bucketsdataset_bin[&#x27;fnlwgt&#x27;] = pd.cut(dataset_raw[&#x27;fnlwgt&#x27;], 10)dataset_con[&#x27;fnlwgt&#x27;] = dataset_raw[&#x27;fnlwgt&#x27;] 123plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,4)) sns.countplot(y=&quot;fnlwgt&quot;, data=dataset_bin); Feature: Education Number1234567# Let&#x27;s use the Pandas Cut function to bin the data in equally sized bucketsdataset_bin[&#x27;education-num&#x27;] = pd.cut(dataset_raw[&#x27;education-num&#x27;], 10)dataset_con[&#x27;education-num&#x27;] = dataset_raw[&#x27;education-num&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,5)) sns.countplot(y=&quot;education-num&quot;, data=dataset_bin); Feature: Hours per Week12345678910# Let&#x27;s use the Pandas Cut function to bin the data in equally sized bucketsdataset_bin[&#x27;hours-per-week&#x27;] = pd.cut(dataset_raw[&#x27;hours-per-week&#x27;], 10)dataset_con[&#x27;hours-per-week&#x27;] = dataset_raw[&#x27;hours-per-week&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,4)) plt.subplot(1, 2, 1)sns.countplot(y=&quot;hours-per-week&quot;, data=dataset_bin);plt.subplot(1, 2, 2)sns.distplot(dataset_con[&#x27;hours-per-week&#x27;]); Feature: Capital Gain12345678910# Let&#x27;s use the Pandas Cut function to bin the data in equally sized bucketsdataset_bin[&#x27;capital-gain&#x27;] = pd.cut(dataset_raw[&#x27;capital-gain&#x27;], 5)dataset_con[&#x27;capital-gain&#x27;] = dataset_raw[&#x27;capital-gain&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,3)) plt.subplot(1, 2, 1)sns.countplot(y=&quot;capital-gain&quot;, data=dataset_bin);plt.subplot(1, 2, 2)sns.distplot(dataset_con[&#x27;capital-gain&#x27;]); Feature: Capital Loss12345678910# Let&#x27;s use the Pandas Cut function to bin the data in equally sized bucketsdataset_bin[&#x27;capital-loss&#x27;] = pd.cut(dataset_raw[&#x27;capital-loss&#x27;], 5)dataset_con[&#x27;capital-loss&#x27;] = dataset_raw[&#x27;capital-loss&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,3)) plt.subplot(1, 2, 1)sns.countplot(y=&quot;capital-loss&quot;, data=dataset_bin);plt.subplot(1, 2, 2)sns.distplot(dataset_con[&#x27;capital-loss&#x27;]); Features: Race, Sex, Relationship1234# Some features we&#x27;ll consider to be in good enough shape as to pass throughdataset_con[&#x27;sex&#x27;] = dataset_bin[&#x27;sex&#x27;] = dataset_raw[&#x27;sex&#x27;]dataset_con[&#x27;race&#x27;] = dataset_bin[&#x27;race&#x27;] = dataset_raw[&#x27;race&#x27;]dataset_con[&#x27;relationship&#x27;] = dataset_bin[&#x27;relationship&#x27;] = dataset_raw[&#x27;relationship&#x27;] Bi-variate AnalysisSo far, we have analised all features individually. Let’s now start combining some of these features together to obtain further insight into the interactions between them. 12345678910111213141516# Plot a count of the categories from each categorical feature split by our prediction class: salary - predclass.def plot_bivariate_bar(dataset, hue, cols=5, width=20, height=15, hspace=0.2, wspace=0.5): dataset = dataset.select_dtypes(include=[np.object]) plt.style.use(&#x27;seaborn-whitegrid&#x27;) fig = plt.figure(figsize=(width,height)) fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace) rows = math.ceil(float(dataset.shape[1]) / cols) for i, column in enumerate(dataset.columns): ax = fig.add_subplot(rows, cols, i + 1) ax.set_title(column) if dataset.dtypes[column] == np.object: g = sns.countplot(y=column, hue=hue, data=dataset) substrings = [s.get_text()[:10] for s in g.get_yticklabels()] g.set(yticklabels=substrings) plot_bivariate_bar(dataset_con, hue=&#x27;predclass&#x27;, cols=3, width=20, height=12, hspace=0.4, wspace=0.5) 1234# Effect of Marital Status and Education on Income, across Marital Status.plt.style.use(&#x27;seaborn-whitegrid&#x27;)g = sns.FacetGrid(dataset_con, col=&#x27;marital-status&#x27;, size=4, aspect=.7)g = g.map(sns.boxplot, &#x27;predclass&#x27;, &#x27;education-num&#x27;) 1234567891011# Historical Trends on the Sex, Education, HPW and Age impact on Income.plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,4)) plt.subplot(1, 3, 1)sns.violinplot(x=&#x27;sex&#x27;, y=&#x27;education-num&#x27;, hue=&#x27;predclass&#x27;, data=dataset_con, split=True, scale=&#x27;count&#x27;);plt.subplot(1, 3, 2)sns.violinplot(x=&#x27;sex&#x27;, y=&#x27;hours-per-week&#x27;, hue=&#x27;predclass&#x27;, data=dataset_con, split=True, scale=&#x27;count&#x27;);plt.subplot(1, 3, 3)sns.violinplot(x=&#x27;sex&#x27;, y=&#x27;age&#x27;, hue=&#x27;predclass&#x27;, data=dataset_con, split=True, scale=&#x27;count&#x27;); 12345# Interaction between pairs of features.sns.pairplot(dataset_con[[&#x27;age&#x27;,&#x27;education-num&#x27;,&#x27;hours-per-week&#x27;,&#x27;predclass&#x27;,&#x27;capital-gain&#x27;,&#x27;capital-loss&#x27;]], hue=&quot;predclass&quot;, diag_kind=&quot;kde&quot;, size=4); Feature Crossing: Age + Hours Per WeekSo far, we have modified and cleaned features that existed in our dataset. However, we can go further and create a new new variables, adding human knowledge on the interaction between features. 12345678910111213# Crossing Numerical Featuresdataset_con[&#x27;age-hours&#x27;] = dataset_con[&#x27;age&#x27;] * dataset_con[&#x27;hours-per-week&#x27;]dataset_bin[&#x27;age-hours&#x27;] = pd.cut(dataset_con[&#x27;age-hours&#x27;], 10)dataset_con[&#x27;age-hours&#x27;] = dataset_con[&#x27;age-hours&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,5)) plt.subplot(1, 2, 1)sns.countplot(y=&quot;age-hours&quot;, data=dataset_bin);plt.subplot(1, 2, 2)sns.distplot(dataset_con.loc[dataset_con[&#x27;predclass&#x27;] == 1][&#x27;age-hours&#x27;], kde_kws=&#123;&quot;label&quot;: &quot;&gt;$50K&quot;&#125;);sns.distplot(dataset_con.loc[dataset_con[&#x27;predclass&#x27;] == 0][&#x27;age-hours&#x27;], kde_kws=&#123;&quot;label&quot;: &quot;&lt;$50K&quot;&#125;); 123456# Crossing Categorical Featuresdataset_bin[&#x27;sex-marital&#x27;] = dataset_con[&#x27;sex-marital&#x27;] = dataset_con[&#x27;sex&#x27;] + dataset_con[&#x27;marital-status&#x27;]plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(20,5)) sns.countplot(y=&quot;sex-marital&quot;, data=dataset_bin); Feature EncodingRemember that Machine Learning algorithms perform Linear Algebra on Matrices, which means all features need have numeric values. The process of converting Categorical Features into values is called Encoding. Here only perform One-Hot but not Label encoding. Additional Resources: http://pbpython.com/categorical-encoding.html 123456# One Hot Encodes all labels before Machine Learningone_hot_cols = dataset_bin.columns.tolist()one_hot_cols.remove(&#x27;predclass&#x27;)dataset_bin_enc = pd.get_dummies(dataset_bin, columns=one_hot_cols)dataset_bin_enc.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; predclass age_(16.927, 24.3] age_(24.3, 31.6] age_(31.6, 38.9] age_(38.9, 46.2] age_(46.2, 53.5] age_(53.5, 60.8] age_(60.8, 68.1] age_(68.1, 75.4] age_(75.4, 82.7] ... sex-marital_FemaleMarried sex-marital_FemaleNever-Married sex-marital_FemaleNot-Married sex-marital_FemaleSeparated sex-marital_FemaleWidowed sex-marital_MaleMarried sex-marital_MaleNever-Married sex-marital_MaleNot-Married sex-marital_MaleSeparated sex-marital_MaleWidowed 0 0 0 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 3 0 0 0 0 0 1 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 4 0 0 1 0 0 0 0 0 0 0 ... 1 0 0 0 0 0 0 0 0 0 5 rows × 116 columns 1234567891011121314151617# &#x27;dataset_con&#x27; is original input dataset for this section# build a new dataframe containing only the object columns#obj_df = dataset_con.select_dtypes(include=[&#x27;object&#x27;]).copy()#obj_df.head()# use dropna() delete NaN rows#obj_df = obj_df.dropna(axis=0)# use most prevailing value to fill in the null values# (Private -&gt; NaN workclass)#obj_df[obj_df.isnull().any(axis=1)]#obj_df[&quot;workclass&quot;].value_counts()#obj_df = obj_df.fillna(&#123;&quot;workclass&quot;: &quot;Private&quot;&#125;) 1#dataset_con.dtypes 1234# delete the rows contains NaN valuesdataset_con_enc = dataset_con.dropna(axis=0)print(dataset_con_enc)dataset_con_enc[dataset_con_enc.isnull().any(axis=1)] predclass age workclass occupation native-country education \\ 0 0 39 Non-fed-gov Admin United-States Bachelors 1 0 50 Self-emp Office Labour United-States Bachelors 2 0 38 Private Manual Labour United-States HS-grad 3 0 53 Private Manual Labour United-States Dropout 4 0 28 Private Professional South-America Bachelors ... ... ... ... ... ... ... 48836 0 33 Private Professional United-States Bachelors 48837 0 39 Private Professional United-States Bachelors 48839 0 38 Private Professional United-States Bachelors 48840 0 44 Private Admin United-States Bachelors 48841 1 35 Self-emp Office Labour United-States Bachelors marital-status fnlwgt education-num hours-per-week capital-gain \\ 0 Never-Married 77516 13 40 2174 1 Married 83311 13 13 0 2 Separated 215646 9 40 0 3 Married 234721 7 40 0 4 Married 338409 13 40 0 ... ... ... ... ... ... 48836 Never-Married 245211 13 40 0 48837 Separated 215419 13 36 0 48839 Married 374983 13 50 0 48840 Separated 83891 13 40 5455 48841 Married 182148 13 60 0 capital-loss sex race relationship age-hours \\ 0 0 Male White Not-in-family 1560 1 0 Male White Husband 650 2 0 Male White Not-in-family 1520 3 0 Male Black Husband 2120 4 0 Female Black Wife 1120 ... ... ... ... ... ... 48836 0 Male White Own-child 1320 48837 0 Female White Not-in-family 1404 48839 0 Male White Husband 1900 48840 0 Male Asian-Pac-Islander Own-child 1760 48841 0 Male White Husband 2100 sex-marital 0 MaleNever-Married 1 MaleMarried 2 MaleSeparated 3 MaleMarried 4 FemaleMarried ... ... 48836 MaleNever-Married 48837 FemaleSeparated 48839 MaleMarried 48840 MaleSeparated 48841 MaleMarried [45222 rows x 17 columns] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; predclass age workclass occupation native-country education marital-status fnlwgt education-num hours-per-week capital-gain capital-loss sex race relationship age-hours sex-marital 12345# Label Encode all labelsle = preprocessing.LabelEncoder()dataset_con_enc = dataset_con_enc.apply(le.fit_transform)dataset_con_enc.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; predclass age workclass occupation native-country education marital-status fnlwgt education-num hours-per-week capital-gain capital-loss sex race relationship age-hours sex-marital 0 0 22 1 0 7 1 1 3217 12 39 26 0 1 4 1 655 6 1 0 33 4 3 7 1 0 3519 12 12 0 0 1 4 0 302 5 2 0 21 3 1 7 5 3 17196 8 39 0 0 1 4 1 644 8 3 0 36 3 1 7 3 0 18738 6 39 0 0 1 2 0 847 5 4 0 11 3 4 6 1 0 23828 12 39 0 0 0 2 5 494 0 Feature Reduction / SelectionOnce we have our features ready to use, we might find that the number of features available is too large to be run in a reasonable timeframe by our machine learning algorithms. There’s a number of options available to us for feature reduction and feature selection. Dimensionality Reduction: Principal Component Analysis (PCA): Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. Singular Value Decomposition (SVD): SVD is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m×n matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics. Feature Importance/Relevance: Filter Methods: Filter type methods select features based only on general metrics like the correlation with the variable to predict. Filter methods suppress the least interesting variables. The other variables will be part of a classification or a regression model used to classify or to predict data. These methods are particularly effective in computation time and robust to overfitting. Wrapper Methods: Wrapper methods evaluate subsets of variables which allows, unlike filter approaches, to detect the possible interactions between variables. The two main disadvantages of these methods are : The increasing overfitting risk when the number of observations is insufficient. AND. The significant computation time when the number of variables is large. Embedded Methods: Embedded methods try to combine the advantages of both previous methods. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously. Feature CorrelationCorrelation ia s measure of how much two random variables change together. Features should be uncorrelated with each other and highly correlated to the feature we’re trying to predict. 123456789101112131415161718192021222324# Create a correlation plot of both datasets.plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(25,10)) plt.subplot(1, 2, 1)# Generate a mask for the upper trianglemask = np.zeros_like(dataset_bin_enc.corr(), dtype=np.bool)mask[np.triu_indices_from(mask)] = Truesns.heatmap(dataset_bin_enc.corr(), vmin=-1, vmax=1, square=True, cmap=sns.color_palette(&quot;RdBu_r&quot;, 100), mask=mask, linewidths=.5);plt.subplot(1, 2, 2)mask = np.zeros_like(dataset_con_enc.corr(), dtype=np.bool)mask[np.triu_indices_from(mask)] = Truesns.heatmap(dataset_con_enc.corr(), vmin=-1, vmax=1, square=True, cmap=sns.color_palette(&quot;RdBu_r&quot;, 100), mask=mask, linewidths=.5); Feature ImportanceRandom forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. The measure based on which the (locally) optimal condition is chosen is called impurity. When training a tree, it can be computed how much each feature decreases the weighted impurity in a tree. For a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure. This is the feature importance measure exposed in sklearn’s Random Forest implementations. 12345678# Using Random Forest to gain an insight on Feature Importanceclf = RandomForestClassifier()clf.fit(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1), dataset_con_enc[&#x27;predclass&#x27;])plt.style.use(&#x27;seaborn-whitegrid&#x27;)importance = clf.feature_importances_importance = pd.DataFrame(importance, index=dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1).columns, columns=[&quot;Importance&quot;])importance.sort_values(by=&#x27;Importance&#x27;, ascending=True).plot(kind=&#x27;barh&#x27;, figsize=(20,len(importance)/2)); PCAPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. We can use PCA to reduce the number of features to use in our ML algorithms, and graphing the variance gives us an idea of how many features we really need to represent our dataset fully. 1234567891011121314151617181920212223242526# Calculating PCA for both datasets, and graphing the Variance for each feature, per datasetstd_scale = preprocessing.StandardScaler().fit(dataset_bin_enc.drop(&#x27;predclass&#x27;, axis=1))X = std_scale.transform(dataset_bin_enc.drop(&#x27;predclass&#x27;, axis=1))pca1 = PCA(n_components=len(dataset_bin_enc.columns)-1)fit1 = pca1.fit(X)std_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1))X = std_scale.transform(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1))pca2 = PCA(n_components=len(dataset_con_enc.columns)-2)fit2 = pca2.fit(X)# Graphing the variance per featureplt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(25,7)) plt.subplot(1, 2, 1)plt.xlabel(&#x27;PCA Feature&#x27;)plt.ylabel(&#x27;Variance&#x27;)plt.title(&#x27;PCA for Discretised Dataset&#x27;)plt.bar(range(0, fit1.explained_variance_ratio_.size), fit1.explained_variance_ratio_);plt.subplot(1, 2, 2)plt.xlabel(&#x27;PCA Feature&#x27;)plt.ylabel(&#x27;Variance&#x27;)plt.title(&#x27;PCA for Continuous Dataset&#x27;)plt.bar(range(0, fit2.explained_variance_ratio_.size), fit2.explained_variance_ratio_); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# PCA&#x27;s components graphed in 2D and 3D# Apply Scaling std_scale = preprocessing.StandardScaler().fit(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1))X = std_scale.transform(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1))y = dataset_con_enc[&#x27;predclass&#x27;]# Formattingtarget_names = [0,1]colors = [&#x27;navy&#x27;,&#x27;darkorange&#x27;]lw = 2alpha = 0.3# 2 Components PCAplt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(2, figsize=(20, 8))plt.subplot(1, 2, 1)pca = PCA(n_components=2)X_r = pca.fit(X).transform(X)for color, i, target_name in zip(colors, [0, 1], target_names): plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=alpha, lw=lw, label=target_name)plt.legend(loc=&#x27;best&#x27;, shadow=False, scatterpoints=1)plt.title(&#x27;First two PCA directions&#x27;);# 3 Components PCAax = plt.subplot(1, 2, 2, projection=&#x27;3d&#x27;)pca = PCA(n_components=3)X_reduced = pca.fit(X).transform(X)for color, i, target_name in zip(colors, [0, 1], target_names): ax.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], X_reduced[y == i, 2], color=color, alpha=alpha, lw=lw, label=target_name)plt.legend(loc=&#x27;best&#x27;, shadow=False, scatterpoints=1)ax.set_title(&quot;First three PCA directions&quot;)ax.set_xlabel(&quot;1st eigenvector&quot;)ax.set_ylabel(&quot;2nd eigenvector&quot;)ax.set_zlabel(&quot;3rd eigenvector&quot;)# rotate the axesax.view_init(30, 10) Recursive Feature EliminationFeature ranking with recursive feature elimination and cross-validated selection of the best number of features. 1234567891011121314# Calculating RFE for non-discretised dataset, and graphing the Importance for each feature, per datasetselector1 = RFECV(LogisticRegression(), step=1, cv=5, n_jobs=-1)selector1 = selector1.fit(dataset_con_enc.drop(&#x27;predclass&#x27;, axis=1).values, dataset_con_enc[&#x27;predclass&#x27;].values)print(&quot;Feature Ranking For Non-Discretised: %s&quot; % selector1.ranking_)print(&quot;Optimal number of features : %d&quot; % selector1.n_features_)# Plot number of features VS. cross-validation scoresplt.style.use(&#x27;seaborn-whitegrid&#x27;)plt.figure(figsize=(20,5)) plt.xlabel(&quot;Number of features selected - Non-Discretised&quot;)plt.ylabel(&quot;Cross validation score (nb of correct classifications)&quot;)plt.plot(range(1, len(selector1.grid_scores_) + 1), selector1.grid_scores_);# Feature space could be subsetted like so:dataset_con_enc = dataset_con_enc[dataset_con_enc.columns[np.insert(selector1.support_, 0, True)]] Feature Ranking For Non-Discretised: [1 1 1 1 3 1 4 1 1 1 1 1 1 1 2 1] Optimal number of features : 13 Selecting DatasetWe now have two datasets to choose from to apply our ML algorithms. The one-hot-encoded, and the label-encoded. For now, we have decided not to use feature reduction or selection algorithms. 1234567# OPTIONS: # - dataset_bin_enc# - dataset_con_enc# Change the dataset to test how would the algorithms perform under a differently encoded dataset.selected_dataset = dataset_bin_enc 1selected_dataset.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; predclass age_(16.927, 24.3] age_(24.3, 31.6] age_(31.6, 38.9] age_(38.9, 46.2] age_(46.2, 53.5] age_(53.5, 60.8] age_(60.8, 68.1] age_(68.1, 75.4] age_(75.4, 82.7] ... sex-marital_FemaleMarried sex-marital_FemaleNever-Married sex-marital_FemaleNot-Married sex-marital_FemaleSeparated sex-marital_FemaleWidowed sex-marital_MaleMarried sex-marital_MaleNever-Married sex-marital_MaleNot-Married sex-marital_MaleSeparated sex-marital_MaleWidowed 0 0 0 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 2 rows × 116 columns Splitting Data into Training and Testing DatasetsWe need to split the data back into the training and testing datasets. Remember we joined both right at the beginning. 123# Splitting the Training and Test data setstrain = selected_dataset.loc[0:32560,:]test = selected_dataset.loc[32560:,:] Removing Samples with Missing dataWe could have removed rows with missing data during feature cleaning, but we’re choosing to do it at this point. It’s easier to do it this way, right after we split the data into Training and Testing. Otherwise we would have had to keep track of the number of deleted rows in our data and take that into account when deciding on a splitting boundary for our joined data. 1234# Given missing fields are a small percentange of the overall dataset, # we have chosen to delete them.train = train.dropna(axis=0)test = test.dropna(axis=0) Rename datasets before Machine Learning algos12345X_train_w_label = trainX_train = train.drop([&#x27;predclass&#x27;], axis=1)y_train = train[&#x27;predclass&#x27;].astype(&#x27;int64&#x27;)X_test = test.drop([&#x27;predclass&#x27;], axis=1)y_test = test[&#x27;predclass&#x27;].astype(&#x27;int64&#x27;) Machine Learning AlgorithmsData ReviewLet’s take one last peek at our data before we start running the Machine Learning algorithms. 1X_train.shape (32561, 115) 1X_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; age_(16.927, 24.3] age_(24.3, 31.6] age_(31.6, 38.9] age_(38.9, 46.2] age_(46.2, 53.5] age_(53.5, 60.8] age_(60.8, 68.1] age_(68.1, 75.4] age_(75.4, 82.7] age_(82.7, 90.0] ... sex-marital_FemaleMarried sex-marital_FemaleNever-Married sex-marital_FemaleNot-Married sex-marital_FemaleSeparated sex-marital_FemaleWidowed sex-marital_MaleMarried sex-marital_MaleNever-Married sex-marital_MaleNot-Married sex-marital_MaleSeparated sex-marital_MaleWidowed 0 0 0 0 1 0 0 0 0 0 0 ... 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 0 3 0 0 0 0 1 0 0 0 0 0 ... 0 0 0 0 0 1 0 0 0 0 4 0 1 0 0 0 0 0 0 0 0 ... 1 0 0 0 0 0 0 0 0 0 5 rows × 115 columns 1y_train.head() 0 0 1 0 2 0 3 0 4 0 Name: predclass, dtype: int64 123# Setting a random seed will guarantee we get the same results # every time we run our training and testing.random.seed(1) AlgorithmsFrom here, we will be running the following algorithms. KNN Logistic Regression Random Forest Naive Bayes Stochastic Gradient Decent Linear SVC Decision Tree Gradient Boosted Trees Because there’s a great deal of repetitiveness on the code for each, we’ll create a custom function to analyse this. For some algorithms, we have also chosen to run a Random Hyperparameter search, to select the best hyperparameters for a given algorithm. 12345678910111213# calculate the fpr and tpr for all thresholds of the classificationdef plot_roc_curve(y_test, preds): fpr, tpr, threshold = metrics.roc_curve(y_test, preds) roc_auc = metrics.auc(fpr, tpr) plt.title(&#x27;Receiver Operating Characteristic&#x27;) plt.plot(fpr, tpr, &#x27;b&#x27;, label = &#x27;AUC = %0.2f&#x27; % roc_auc) plt.legend(loc = &#x27;lower right&#x27;) plt.plot([0, 1], [0, 1],&#x27;r--&#x27;) plt.xlim([-0.01, 1.01]) plt.ylim([-0.01, 1.01]) plt.ylabel(&#x27;True Positive Rate&#x27;) plt.xlabel(&#x27;False Positive Rate&#x27;) plt.show() 1234567891011121314151617181920212223# Function that runs the requested algorithm and returns the accuracy metricsdef fit_ml_algo(algo, X_train, y_train, X_test, cv): # One Pass model = algo.fit(X_train, y_train) test_pred = model.predict(X_test) if (isinstance(algo, (LogisticRegression, KNeighborsClassifier, GaussianNB, DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier))): probs = model.predict_proba(X_test)[:,1] else: probs = &quot;Not Available&quot; acc = round(model.score(X_test, y_test) * 100, 2) # CV train_pred = model_selection.cross_val_predict(algo, X_train, y_train, cv=cv, n_jobs = -1) acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2) return train_pred, test_pred, acc, acc_cv, probs 123456789101112131415161718192021222324252627282930313233# Logistic Regression - Random Search for Hyperparameters# Utility function to report best scoresdef report(results, n_top=5): for i in range(1, n_top + 1): candidates = np.flatnonzero(results[&#x27;rank_test_score&#x27;] == i) for candidate in candidates: print(&quot;Model with rank: &#123;0&#125;&quot;.format(i)) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&#x27;mean_test_score&#x27;][candidate], results[&#x27;std_test_score&#x27;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&#x27;params&#x27;][candidate])) print(&quot;&quot;) # Specify parameters and distributions to sample fromparam_dist = &#123;&#x27;penalty&#x27;: [&#x27;l2&#x27;, &#x27;l1&#x27;], &#x27;class_weight&#x27;: [None, &#x27;balanced&#x27;], &#x27;C&#x27;: np.logspace(-20, 20, 10000), &#x27;intercept_scaling&#x27;: np.logspace(-20, 20, 10000)&#125;# Run Randomized Searchn_iter_search = 10lrc = LogisticRegression()random_search = RandomizedSearchCV(lrc, n_jobs=-1, param_distributions=param_dist, n_iter=n_iter_search)start = time.time()random_search.fit(X_train, y_train)print(&quot;RandomizedSearchCV took %.2f seconds for %d candidates&quot; &quot; parameter settings.&quot; % ((time.time() - start), n_iter_search))report(random_search.cv_results_) RandomizedSearchCV took 6.84 seconds for 10 candidates parameter settings. Model with rank: 1 Mean validation score: 0.844 (std: 0.004) Parameters: &#123;&#39;penalty&#39;: &#39;l2&#39;, &#39;intercept_scaling&#39;: 42370413880.09742, &#39;class_weight&#39;: None, &#39;C&#39;: 6.248554728170629e+17&#125; Model with rank: 2 Mean validation score: 0.800 (std: 0.004) Parameters: &#123;&#39;penalty&#39;: &#39;l2&#39;, &#39;intercept_scaling&#39;: 5.356398592977186e-12, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;C&#39;: 318529980510.9508&#125; Model with rank: 2 Mean validation score: 0.800 (std: 0.004) Parameters: &#123;&#39;penalty&#39;: &#39;l2&#39;, &#39;intercept_scaling&#39;: 6.741908876164404e-13, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;C&#39;: 1.753171420878381e+18&#125; Model with rank: 4 Mean validation score: 0.800 (std: 0.004) Parameters: &#123;&#39;penalty&#39;: &#39;l2&#39;, &#39;intercept_scaling&#39;: 0.03646331805309427, &#39;class_weight&#39;: &#39;balanced&#39;, &#39;C&#39;: 431085.5408791511&#125; Model with rank: 5 Mean validation score: 0.759 (std: 0.000) Parameters: &#123;&#39;penalty&#39;: &#39;l2&#39;, &#39;intercept_scaling&#39;: 52853324182.66478, &#39;class_weight&#39;: None, &#39;C&#39;: 3.311707756163145e-20&#125; 1234567891011# Logistic Regressionstart_time = time.time()train_pred_log, test_pred_log, acc_log, acc_cv_log, probs_log = fit_ml_algo(LogisticRegression(n_jobs = -1), X_train, y_train, X_test, 10)log_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_log)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_log)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=log_time)) Accuracy: 84.47 Accuracy CV 10-Fold: 84.33 Running Time: 0:00:09.857440 1print(metrics.confusion_matrix(y_test, test_pred_log)) [[11501 934] [ 1595 2252]] 1print(metrics.classification_report(y_train, train_pred_log)) precision recall f1-score support 0 0.88 0.93 0.90 24720 1 0.71 0.58 0.64 7841 accuracy 0.84 32561 macro avg 0.79 0.75 0.77 32561 weighted avg 0.84 0.84 0.84 32561 1print(metrics.classification_report(y_test, test_pred_log)) precision recall f1-score support 0 0.88 0.92 0.90 12435 1 0.71 0.59 0.64 3847 accuracy 0.84 16282 macro avg 0.79 0.76 0.77 16282 weighted avg 0.84 0.84 0.84 16282 1plot_roc_curve(y_test, probs_log) 123456789101112# k-Nearest Neighborsstart_time = time.time()train_pred_knn, test_pred_knn, acc_knn, acc_cv_knn, probs_knn = fit_ml_algo(KNeighborsClassifier(n_neighbors = 3, n_jobs = -1), X_train, y_train, X_test, 10)knn_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_knn)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_knn)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=knn_time)) Accuracy: 81.02 Accuracy CV 10-Fold: 81.13 Running Time: 0:02:21.181324 1print(metrics.classification_report(y_train, train_pred_knn)) precision recall f1-score support 0 0.86 0.89 0.88 24720 1 0.62 0.56 0.59 7841 accuracy 0.81 32561 macro avg 0.74 0.73 0.73 32561 weighted avg 0.81 0.81 0.81 32561 1print(metrics.classification_report(y_test, test_pred_knn)) precision recall f1-score support 0 0.87 0.89 0.88 12435 1 0.61 0.56 0.58 3847 accuracy 0.81 16282 macro avg 0.74 0.72 0.73 16282 weighted avg 0.81 0.81 0.81 16282 1plot_roc_curve(y_test, probs_knn) 1234567891011# Gaussian Naive Bayesstart_time = time.time()train_pred_gaussian, test_pred_gaussian, acc_gaussian, acc_cv_gaussian, probs_gau = fit_ml_algo(GaussianNB(), X_train, y_train, X_test, 10)gaussian_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_gaussian)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_gaussian)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=gaussian_time)) Accuracy: 75.59 Accuracy CV 10-Fold: 74.51 Running Time: 0:00:00.479271 1print(metrics.classification_report(y_train, train_pred_gaussian)) precision recall f1-score support 0 0.95 0.70 0.81 24720 1 0.48 0.88 0.62 7841 accuracy 0.75 32561 macro avg 0.72 0.79 0.72 32561 weighted avg 0.84 0.75 0.76 32561 1print(metrics.classification_report(y_test, test_pred_gaussian)) precision recall f1-score support 0 0.94 0.72 0.82 12435 1 0.49 0.86 0.63 3847 accuracy 0.76 16282 macro avg 0.72 0.79 0.72 16282 weighted avg 0.84 0.76 0.77 16282 1plot_roc_curve(y_test, probs_gau) 1234567891011# Linear SVCstart_time = time.time()train_pred_svc, test_pred_svc, acc_linear_svc, acc_cv_linear_svc, _ = fit_ml_algo(LinearSVC(), X_train, y_train, X_test, 10)linear_svc_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_linear_svc)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_linear_svc)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=linear_svc_time)) Accuracy: 84.42 Accuracy CV 10-Fold: 84.46 Running Time: 0:00:07.630441 1print(metrics.classification_report(y_train, train_pred_svc)) precision recall f1-score support 0 0.88 0.93 0.90 24720 1 0.72 0.58 0.64 7841 accuracy 0.84 32561 macro avg 0.80 0.76 0.77 32561 weighted avg 0.84 0.84 0.84 32561 1print(metrics.classification_report(y_test, test_pred_svc)) precision recall f1-score support 0 0.88 0.93 0.90 12435 1 0.71 0.58 0.64 3847 accuracy 0.84 16282 macro avg 0.79 0.75 0.77 16282 weighted avg 0.84 0.84 0.84 16282 1234567891011# Stochastic Gradient Descentstart_time = time.time()train_pred_sgd, test_pred_sgd, acc_sgd, acc_cv_sgd, _ = fit_ml_algo(SGDClassifier(n_jobs = -1), X_train, y_train, X_test, 10)sgd_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_sgd)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_sgd)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=sgd_time)) Accuracy: 84.15 Accuracy CV 10-Fold: 83.74 Running Time: 0:00:02.039138 1print(metrics.classification_report(y_train, train_pred_sgd)) precision recall f1-score support 0 0.88 0.91 0.89 24720 1 0.69 0.60 0.64 7841 accuracy 0.84 32561 macro avg 0.78 0.76 0.77 32561 weighted avg 0.83 0.84 0.83 32561 1print(metrics.classification_report(y_test, test_pred_sgd)) precision recall f1-score support 0 0.88 0.91 0.90 12435 1 0.69 0.61 0.64 3847 accuracy 0.84 16282 macro avg 0.78 0.76 0.77 16282 weighted avg 0.84 0.84 0.84 16282 1234567891011# Decision Tree Classifierstart_time = time.time()train_pred_dt, test_pred_dt, acc_dt, acc_cv_dt, probs_dt = fit_ml_algo(DecisionTreeClassifier(), X_train, y_train, X_test, 10)dt_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_dt)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_dt)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=dt_time)) Accuracy: 79.93 Accuracy CV 10-Fold: 80.44 Running Time: 0:00:01.417276 1print(metrics.confusion_matrix(y_test, test_pred_dt)) [[10956 1479] [ 1788 2059]] 1print(metrics.classification_report(y_train, train_pred_dt)) precision recall f1-score support 0 0.86 0.89 0.87 24720 1 0.60 0.54 0.57 7841 accuracy 0.80 32561 macro avg 0.73 0.72 0.72 32561 weighted avg 0.80 0.80 0.80 32561 1print(metrics.classification_report(y_test, test_pred_dt)) precision recall f1-score support 0 0.86 0.88 0.87 12435 1 0.58 0.54 0.56 3847 accuracy 0.80 16282 macro avg 0.72 0.71 0.71 16282 weighted avg 0.79 0.80 0.80 16282 1plot_roc_curve(y_test, probs_dt) 1234567891011121314151617181920212223242526272829303132333435# Random Forest Classifier - Random Search for Hyperparameters# Utility function to report best scoresdef report(results, n_top=5): for i in range(1, n_top + 1): candidates = np.flatnonzero(results[&#x27;rank_test_score&#x27;] == i) for candidate in candidates: print(&quot;Model with rank: &#123;0&#125;&quot;.format(i)) print(&quot;Mean validation score: &#123;0:.3f&#125; (std: &#123;1:.3f&#125;)&quot;.format( results[&#x27;mean_test_score&#x27;][candidate], results[&#x27;std_test_score&#x27;][candidate])) print(&quot;Parameters: &#123;0&#125;&quot;.format(results[&#x27;params&#x27;][candidate])) print(&quot;&quot;) # Specify parameters and distributions to sample fromparam_dist = &#123;&quot;max_depth&quot;: [10, None], &quot;max_features&quot;: sp_randint(1, 11), &quot;min_samples_split&quot;: sp_randint(2, 20), &quot;min_samples_leaf&quot;: sp_randint(1, 11), &quot;bootstrap&quot;: [True, False], &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;]&#125;# Run Randomized Searchn_iter_search = 10rfc = RandomForestClassifier(n_estimators=10)random_search = RandomizedSearchCV(rfc, n_jobs = -1, param_distributions=param_dist, n_iter=n_iter_search)start = time.time()random_search.fit(X_train, y_train)print(&quot;RandomizedSearchCV took %.2f seconds for %d candidates&quot; &quot; parameter settings.&quot; % ((time.time() - start), n_iter_search))report(random_search.cv_results_) RandomizedSearchCV took 2.68 seconds for 10 candidates parameter settings. Model with rank: 1 Mean validation score: 0.839 (std: 0.004) Parameters: &#123;&#39;bootstrap&#39;: False, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: 4, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 13&#125; Model with rank: 2 Mean validation score: 0.838 (std: 0.005) Parameters: &#123;&#39;bootstrap&#39;: True, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: 10, &#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 2&#125; Model with rank: 3 Mean validation score: 0.838 (std: 0.005) Parameters: &#123;&#39;bootstrap&#39;: False, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 9, &#39;min_samples_split&#39;: 4&#125; Model with rank: 4 Mean validation score: 0.838 (std: 0.004) Parameters: &#123;&#39;bootstrap&#39;: True, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: 10, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 13&#125; Model with rank: 5 Mean validation score: 0.834 (std: 0.004) Parameters: &#123;&#39;bootstrap&#39;: False, &#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: 7, &#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 2&#125; 12345678910111213141516# Random Forest Classifierstart_time = time.time()rfc = RandomForestClassifier(n_estimators=10, min_samples_leaf=2, min_samples_split=17, criterion=&#x27;gini&#x27;, max_features=8)train_pred_rf, test_pred_rf, acc_rf, acc_cv_rf, probs_rf = fit_ml_algo(rfc, X_train, y_train, X_test, 10)rf_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_rf)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_rf)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=rf_time)) Accuracy: 84.07 Accuracy CV 10-Fold: 84.05 Running Time: 0:00:01.423032 1print(metrics.classification_report(y_train, train_pred_rf)) precision recall f1-score support 0 0.87 0.93 0.90 24720 1 0.71 0.57 0.63 7841 accuracy 0.84 32561 macro avg 0.79 0.75 0.76 32561 weighted avg 0.83 0.84 0.83 32561 1print(metrics.classification_report(y_test, test_pred_rf)) precision recall f1-score support 0 0.87 0.93 0.90 12435 1 0.70 0.56 0.63 3847 accuracy 0.84 16282 macro avg 0.79 0.74 0.76 16282 weighted avg 0.83 0.84 0.83 16282 1plot_roc_curve(y_test, probs_rf) 1234567891011# Gradient Boosting Treesstart_time = time.time()train_pred_gbt, test_pred_gbt, acc_gbt, acc_cv_gbt, probs_gbt = fit_ml_algo(GradientBoostingClassifier(), X_train, y_train, X_test, 10)gbt_time = (time.time() - start_time)print(&quot;Accuracy: %s&quot; % acc_gbt)print(&quot;Accuracy CV 10-Fold: %s&quot; % acc_cv_gbt)print(&quot;Running Time: %s&quot; % datetime.timedelta(seconds=gbt_time)) Accuracy: 84.53 Accuracy CV 10-Fold: 84.34 Running Time: 0:00:18.993168 1print(metrics.classification_report(y_train, train_pred_gbt)) precision recall f1-score support 0 0.87 0.93 0.90 24720 1 0.72 0.57 0.64 7841 accuracy 0.84 32561 macro avg 0.80 0.75 0.77 32561 weighted avg 0.84 0.84 0.84 32561 1print(metrics.classification_report(y_test, test_pred_gbt)) precision recall f1-score support 0 0.88 0.93 0.90 12435 1 0.71 0.58 0.64 3847 accuracy 0.85 16282 macro avg 0.79 0.75 0.77 16282 weighted avg 0.84 0.85 0.84 16282 1plot_roc_curve(y_test, probs_gbt) Ranking ResultsLet’s rank the results for all the algorithms we have used 12345678910111213141516models = pd.DataFrame(&#123; &#x27;Model&#x27;: [&#x27;KNN&#x27;, &#x27;Logistic Regression&#x27;, &#x27;Random Forest&#x27;, &#x27;Naive Bayes&#x27;, &#x27;Stochastic Gradient Decent&#x27;, &#x27;Linear SVC&#x27;, &#x27;Decision Tree&#x27;, &#x27;Gradient Boosting Trees&#x27;], &#x27;Score&#x27;: [ acc_knn, acc_log, acc_rf, acc_gaussian, acc_sgd, acc_linear_svc, acc_dt, acc_gbt ]&#125;)models.sort_values(by=&#x27;Score&#x27;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Model Score 7 Gradient Boosting Trees 84.53 1 Logistic Regression 84.47 5 Linear SVC 84.42 4 Stochastic Gradient Decent 84.15 2 Random Forest 84.07 0 KNN 81.02 6 Decision Tree 79.93 3 Naive Bayes 75.59 12345678910111213141516models = pd.DataFrame(&#123; &#x27;Model&#x27;: [&#x27;KNN&#x27;, &#x27;Logistic Regression&#x27;, &#x27;Random Forest&#x27;, &#x27;Naive Bayes&#x27;, &#x27;Stochastic Gradient Decent&#x27;, &#x27;Linear SVC&#x27;, &#x27;Decision Tree&#x27;, &#x27;Gradient Boosting Trees&#x27;], &#x27;Score&#x27;: [ acc_cv_knn, acc_cv_log, acc_cv_rf, acc_cv_gaussian, acc_cv_sgd, acc_cv_linear_svc, acc_cv_dt, acc_cv_gbt ]&#125;)models.sort_values(by=&#x27;Score&#x27;, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Model Score 5 Linear SVC 84.46 7 Gradient Boosting Trees 84.34 1 Logistic Regression 84.33 2 Random Forest 84.05 4 Stochastic Gradient Decent 83.74 0 KNN 81.13 6 Decision Tree 80.44 3 Naive Bayes 74.51 123456789101112131415161718192021222324252627282930313233343536373839404142434445plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(10,10)) models = [ &#x27;KNN&#x27;, &#x27;Logistic Regression&#x27;, &#x27;Random Forest&#x27;, &#x27;Naive Bayes&#x27;, &#x27;Decision Tree&#x27;, &#x27;Gradient Boosting Trees&#x27;]probs = [ probs_knn, probs_log, probs_rf, probs_gau, probs_dt, probs_gbt]colors = [ &#x27;blue&#x27;, &#x27;green&#x27;, &#x27;red&#x27;, &#x27;cyan&#x27;, &#x27;magenta&#x27;, &#x27;yellow&#x27;,] plt.title(&#x27;Receiver Operating Characteristic&#x27;)plt.plot([0, 1], [0, 1],&#x27;r--&#x27;)plt.xlim([-0.01, 1.01])plt.ylim([-0.01, 1.01])plt.ylabel(&#x27;True Positive Rate&#x27;)plt.xlabel(&#x27;False Positive Rate&#x27;)def plot_roc_curves(y_test, prob, model): fpr, tpr, threshold = metrics.roc_curve(y_test, prob) roc_auc = metrics.auc(fpr, tpr) plt.plot(fpr, tpr, &#x27;b&#x27;, label = model + &#x27; AUC = %0.2f&#x27; % roc_auc, color=colors[i]) plt.legend(loc = &#x27;lower right&#x27;) for i, model in list(enumerate(models)): plot_roc_curves(y_test, probs[i], models[i]) plt.show() 123456789101112131415161718192021222324252627282930313233plt.style.use(&#x27;seaborn-whitegrid&#x27;)fig = plt.figure(figsize=(10,10)) models = [ &#x27;Logistic Regression&#x27;, &#x27;Decision Tree&#x27;, ]probs = [ probs_log, probs_dt,]colors = [ &#x27;blue&#x27;, &#x27;green&#x27;,] plt.title(&#x27;Receiver Operating Characteristic&#x27;)plt.plot([0, 1], [0, 1],&#x27;r--&#x27;)plt.xlim([-0.01, 1.01])plt.ylim([-0.01, 1.01])plt.ylabel(&#x27;True Positive Rate&#x27;)plt.xlabel(&#x27;False Positive Rate&#x27;)def plot_roc_curves(y_test, prob, model): fpr, tpr, threshold = metrics.roc_curve(y_test, prob) roc_auc = metrics.auc(fpr, tpr) plt.plot(fpr, tpr, &#x27;b&#x27;, label = model + &#x27; AUC = %0.2f&#x27; % roc_auc, color=colors[i]) plt.legend(loc = &#x27;lower right&#x27;) for i, model in list(enumerate(models)): plot_roc_curves(y_test, probs[i], models[i]) plt.show() 1","categories":[{"name":"Juypter","slug":"Juypter","permalink":"http://muyuhuatang.github.io/categories/Juypter/"}],"tags":[]},{"title":"Small Tools","slug":"SmallTools","date":"2020-10-06T13:48:00.590Z","updated":"2020-11-12T12:21:27.905Z","comments":true,"path":"2020/10/06/SmallTools/","link":"","permalink":"http://muyuhuatang.github.io/2020/10/06/SmallTools/","excerpt":"Some interesting small tools","text":"Some interesting small tools Use small software to use netease music totally freeuse the code below to open the local service 1$ node app.js -p 80:443 -f 223.252.199.66 nondanee/UnblockNeteaseMusic Mac implementationStep: open the certificate “UnblockNeteaseMusic Root CA” in Keychain Access. terminal to service directory and use1node app.js -p 80:443 -f 223.252.199.66 end the terminal and untrust the certificate.note: use the ip in “Mac implementation” would be just fine. my ping result always can not connect to music.163.com IOS implementationStep: add the certificate could use and not necessary for pm2 start —— I assume that everybody used the same certificate with built-in https settings. untrust the certificate is fine Reference online ip check","categories":[],"tags":[{"name":"Interesting","slug":"Interesting","permalink":"http://muyuhuatang.github.io/tags/Interesting/"}]},{"title":"LeetCode","slug":"LeetCode","date":"2020-10-04T08:05:51.595Z","updated":"2020-10-09T07:13:25.074Z","comments":true,"path":"2020/10/04/LeetCode/","link":"","permalink":"http://muyuhuatang.github.io/2020/10/04/LeetCode/","excerpt":"LeetCode records (In C++/Python 3)","text":"LeetCode records (In C++/Python 3) LeetCode Hot 100 (In leetcode.cn)(2) Add two numbers12345678910111213141516171819202122232425262728293031class Solution &#123;public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; ListNode *head = nullptr, *tail = nullptr; int carry = 0; while (l1 || l2) &#123; int n1 = l1 ? l1-&gt;val: 0; int n2 = l2 ? l2-&gt;val: 0; int sum = n1 + n2 + carry; if (!head) &#123; head = tail = new ListNode(sum % 10); &#125; else &#123; tail-&gt;next = new ListNode(sum % 10); tail = tail-&gt;next; &#125; carry = sum / 10; if (l1) &#123; l1 = l1-&gt;next; &#125; if (l2) &#123; l2 = l2-&gt;next; &#125; &#125; if (carry &gt; 0) &#123; tail-&gt;next = new ListNode(carry); &#125; return head; &#125;&#125;;Author：LeetCode-Solution 本质: 链表基本知识与操作 + 取余操作 + 利用整数除后省略小数点的特性技巧: 将取余操作与整除特性结合 + 进位符 -&gt; 模拟数学运算 (20) Valid Parentheses12345678910111213141516171819202122232425262728293031class Solution &#123;public: bool isValid(string s) &#123; // simple check int n = s.size(); if (n % 2 == 1) &#123; return false; &#125; // map function. Could also be applied by if unordered_map&lt;char, char&gt; pairs = &#123; &#123;&#x27;)&#x27;, &#x27;(&#x27;&#125;, &#123;&#x27;]&#x27;, &#x27;[&#x27;&#125;, &#123;&#x27;&#125;&#x27;, &#x27;&#123;&#x27;&#125; &#125;; stack&lt;char&gt; stk; for (char ch: s) &#123; if (pairs.count(ch)) &#123; if (stk.empty() || stk.top() != pairs[ch]) &#123; return false; &#125; stk.pop(); &#125; else &#123; stk.push(ch); &#125; &#125; return stk.empty(); &#125;&#125;;Author：LeetCode-Solution 本质: 数据结构“栈”的基础应用 + 使用Hashmap来简化查找匹配问题技巧: 使用hashmap来作为匹配工具 / 适当使用库函数来减少代码并提升可复用性 (1) Two Sum123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt;&amp; nums, int target) &#123; unordered_map&lt;int, int&gt; hashtable; for (int i = 0; i &lt; nums.size(); ++i) &#123; auto it = hashtable.find(target - nums[i]); if (it != hashtable.end()) &#123; // need to return the index in original array return &#123;it-&gt;second, i&#125;; &#125; hashtable[nums[i]] = i; &#125; return &#123;&#125;; &#125;&#125;;Author：LeetCode-Solution 本质 &amp; 技巧: Hash表的使用 Reference LeetCode-cn官网","categories":[{"name":"Data Structure","slug":"Data-Structure","permalink":"http://muyuhuatang.github.io/categories/Data-Structure/"},{"name":"LeetCode","slug":"Data-Structure/LeetCode","permalink":"http://muyuhuatang.github.io/categories/Data-Structure/LeetCode/"}],"tags":[{"name":"Record","slug":"Record","permalink":"http://muyuhuatang.github.io/tags/Record/"}]},{"title":"Data Science","slug":"Data-Science","date":"2020-09-25T12:59:55.053Z","updated":"2020-10-04T08:05:28.232Z","comments":true,"path":"2020/09/25/Data-Science/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/25/Data-Science/","excerpt":"Data Science ralated material","text":"Data Science ralated material 行业分析 工程能力 对数据、对业务的理解 机器学习算法与模型 练习 Kaggle天池比赛 在github上follow一些有意思的ML项目 补充知识 BerkeleyX: CS190.1x Scalable Machine Learning CS105x Introduction to Apache Spark Stanford CS231n Convolutional Neural Networks for Visual Recognition","categories":[{"name":"Data Science","slug":"Data-Science","permalink":"http://muyuhuatang.github.io/categories/Data-Science/"}],"tags":[{"name":"Record","slug":"Record","permalink":"http://muyuhuatang.github.io/tags/Record/"}]},{"title":"Academic Records","slug":"Academic-Records","date":"2020-09-25T12:31:30.908Z","updated":"2020-10-11T13:29:12.732Z","comments":true,"path":"2020/09/25/Academic-Records/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/25/Academic-Records/","excerpt":"Records about useful or intersting academic information","text":"Records about useful or intersting academic information Top conferences and journalsMachine LearningNIPS / ICML / neurips / uai / aistats Computer VisionCVPR / ICCV / ECCV / NIPS Data MiningKDD / ICDM / WSDM / SDM / CIK&lt; Natural Language ProcessingACL / EMNLP / NAACL InformationSIGIR / WWW Deep LearningICLR RoboticICRA / IROS / RSS Artifical IntelligenceAAAI / IJCAI Reference","categories":[{"name":"Documents","slug":"Documents","permalink":"http://muyuhuatang.github.io/categories/Documents/"}],"tags":[{"name":"Record","slug":"Record","permalink":"http://muyuhuatang.github.io/tags/Record/"}]},{"title":"Study Records","slug":"Study-Records","date":"2020-09-25T03:43:44.264Z","updated":"2020-10-11T13:30:20.352Z","comments":true,"path":"2020/09/25/Study-Records/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/25/Study-Records/","excerpt":"Records studying useful knowledge","text":"Records studying useful knowledge 课程知识人工智能 机器学习 数据挖掘 统计学习 intersting small models First Order Motion Model for Image Animation 工程应用 统计与概率 工程设计 探索领域工程开发 核心知识 编程语言: Java/C++ 算法与数据结构 数据库 一门技能课 自我提升 平台Coursera、哈佛公开课、斯坦福公开课 基础课程: 计算机科学导论(Havard) 编程平台: Leetcode 初级课程: 深度学习入门、机器学习入门-Stanford CS229、CV入门-CS231N、NLP入门-CS224N 核心要点: 算法与数据结构: Leetcode刷题 数据库+一门技能课的实战: 网页开发 移动端开发-日历、通信软件 云计算-用spark和Hadoop尝试对大数据进行删改读写 AI-推荐系统、搜索系统、CV、NLP 目标: 大厂面试题对答如流 独立完成3-4个编程项目 人工智能与安全推荐系统","categories":[{"name":"Documents","slug":"Documents","permalink":"http://muyuhuatang.github.io/categories/Documents/"}],"tags":[{"name":"Record","slug":"Record","permalink":"http://muyuhuatang.github.io/tags/Record/"}]},{"title":"Intrusion Detection","slug":"Intrusion-Detection","date":"2020-09-24T14:56:09.992Z","updated":"2020-11-08T16:52:21.548Z","comments":true,"path":"2020/09/24/Intrusion-Detection/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/24/Intrusion-Detection/","excerpt":"Intrusion Detection course","text":"Intrusion Detection course Group Project - NotPetya analysis Interesting questions: shut down the SMBv1 could be a good prevent method, but is there any problems when using SMBv2 and SMBv3? Microsoft network client: Digitally sign communications (always) Cisco Firepower Release Notes, Version 6.2.3 Cisco Firepower System Software Server Message Block File Policy Bypass Vulnerability Prevent Petya and other Ransomware attacks by disabling SMBv1 Here’s why you need to stop using SMBv1 immediately. Even Microsoft agrees. CVE-2018-0243 Detail Research Paperpreparation Github repositries Iot-Cyber-Security-with-Machine-Learning-Research-Project Network-intrusion-system-with-multinomial-classification INTRUSION-DETECTION-USING-MACHINE-LEARNING how to quote the online resource in Paper Experiment Design Convert pcap to csv using the Wireshark Convert csv to txt Apply Network-intrusion-system-with-multinomial-classification for format fitting Use wireshark to generate train and validation data and convert it / checked Implement the existing project If error, try to fix and explain Record the key points when implementing Record the outcomes when running Evaluate the algorithm with validation dataset machine learning regular evaluation Analyze test dataset using toolsMy algorithmother online or existing tools Conclusiongood pointspossible error analysis1) in the music listenning dataset, if I want to use netease music, it need to set a VPN to China to enable it, this may change the attribute features of dataset and cause final classification bias or mistakes. where could be done betterpossible ways to get better1) use tshark or python to extract the csv file from pcap file in a more customizable way 2) config the wireshark to monitor only one application when generating test and validation data. [use wireshark to get specific application&#39;s internet package](https://blog.csdn.net/youxiansanren/article/details/48271851) / go to &quot;Activity Monitor&quot; of Mac to find the pid of firefox browser 441, and port is 31886 / [get the info of the application](https://blog.csdn.net/qq_24909089/article/details/90667898) 3) use the spotfiy Convert pcap to csv download the wireshark from official website / convert pcap to csv / convert csv to txt python ways in CSDN pip install scapy confirm the format of given pcap file123456789101112131415161718192021222324252627282930###[ Ethernet ]### dst &#x3D; 52:54:00:12:35:02 src &#x3D; 08:00:27:92:ab:0d type &#x3D; IPv4###[ IP ]### version &#x3D; 4 ihl &#x3D; 5 tos &#x3D; 0x0 len &#x3D; 40 id &#x3D; 45488 flags &#x3D; DF frag &#x3D; 0 ttl &#x3D; 64 proto &#x3D; tcp chksum &#x3D; 0x7d53 src &#x3D; 10.0.2.15 dst &#x3D; 34.107.221.82 \\options \\###[ TCP ]### sport &#x3D; 35546 dport &#x3D; http seq &#x3D; 2415182248 ack &#x3D; 14528222 dataofs &#x3D; 5 reserved &#x3D; 0 flags &#x3D; A window &#x3D; 64020 chksum &#x3D; 0xbe7 urgptr &#x3D; 0 options &#x3D; [] —–fail! just one row with wrong colums tshark -r sample1.pcap -T fields -e eth.src -e eth.dst -e ip.src -e ip.dst -e ip.proto -E header=y -E separator=, -E quote=d -E occurrence=f &gt; sample1.csv Generate training dataset using wireshark network trouble shotting 1) ping www.google.com 2) traceroute www.google.com 3) ipconfig ifcount 4) nslookup www.google.com 5) whois www.google.com 6) netstat weka reference use weka to predict data / complement of the previews one detailed output file instruction Prepare the data from raw data. add “Class lable”, remember use ‘microsoft excel’ to add coloum but not the ‘mac numbers’ how to use excel to fill one coloum to one value or directly use weka import csv file and convert it to arff file in the ‘info’ attribute, can not contain double quotation marks, which would lead to generation error —– even if choose other method, this would also consequently cause error, so this is a very important tip. replace the string in Excel with empty space-tap a space in the replace textbox of excel but not nothing Dataset preprocessing delete useless attribute ID, which does not contain any information about the intrusion detection process. Reference Converting PCAPs into Weka mineable data - IEEE Xplore Good step instruction —- Machine learning – Network traffic classification using weka Analysis J48 in Weka Use the Wireshark in Mac 10 most used network trouble shooting commands Data Mining with weka (2.2 training and testing) official video guidance Use weka to predict the test data set / Visualization way in weka / Command way in weka Use Lubuntu to test programs in VirtulBox In order to use share function (1) set up shared files (2) when restart Lubuntu, check in terminal “df -l”, and use “sudo mount -t vboxsf [MacSharedFileName] /mnt/shareMac” sudo mount -t vboxsf Shared /mnt/shareMac Install anaconda / Uninstall anaconda APT - advanced persistent threat Wikipedia Refers mainly to hidden and long-term intrusion in computer, which is conducted mainly by professional attackers and aimed at specific targets. The biggest threat is that it is long-term, which means that people can not be certain when did their computers get infected. And that makes the process of recovery even harder and a bit more unnreliable. Security Orchestration, Automation, and Response. official website Famous virusA brief history of Computer Viruses - Youtube Stuxnet wikipedia Reference How to share files between host-mac and client-virtualbox-Lubuntu How to fix ‘virtualbox failed to get display change request’ error / my solution: change the value of video memory of display settings from 16MB to 64MB would make resolution of display become much higher.","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Intrusion Detection","slug":"Courses/Intrusion-Detection","permalink":"http://muyuhuatang.github.io/categories/Courses/Intrusion-Detection/"}],"tags":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"}]},{"title":"Internet Programming","slug":"Internet-Programming","date":"2020-09-24T14:53:15.808Z","updated":"2020-11-03T09:59:57.109Z","comments":true,"path":"2020/09/24/Internet-Programming/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/24/Internet-Programming/","excerpt":"Here's something encrypted, password is required to continue reading. Feel free to email me to get the password with fair reason.","text":"Hey, password is required here.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","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Internet Programming","slug":"Courses/Internet-Programming","permalink":"http://muyuhuatang.github.io/categories/Courses/Internet-Programming/"}],"tags":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"},{"name":"Encrypted","slug":"Encrypted","permalink":"http://muyuhuatang.github.io/tags/Encrypted/"}]},{"title":"Data Mining","slug":"Data-Mining","date":"2020-09-24T14:49:42.263Z","updated":"2020-11-06T11:05:12.626Z","comments":true,"path":"2020/09/24/Data-Mining/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/24/Data-Mining/","excerpt":"Here's something encrypted, password is required to continue reading. Feel free to email me to get the password with fair reason.","text":"Hey, password is required here.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","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Data Mining","slug":"Courses/Data-Mining","permalink":"http://muyuhuatang.github.io/categories/Courses/Data-Mining/"}],"tags":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"},{"name":"Encrypted","slug":"Encrypted","permalink":"http://muyuhuatang.github.io/tags/Encrypted/"}]},{"title":"Text & Web Mining","slug":"Text-Web-Mining","date":"2020-09-24T14:46:56.354Z","updated":"2020-11-22T15:07:14.200Z","comments":true,"path":"2020/09/24/Text-Web-Mining/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/24/Text-Web-Mining/","excerpt":"Text &amp; Web Mining course","text":"Text &amp; Web Mining course Training SkillsHow to use colab to run ML tasks click “Runtime” -&gt; “Change Runtime Type” -&gt; select GPU or TPU use code below to check GPU status!nvidia-smi build connection between google drive files and this notebookfrom google.colab import drivedrive.mount(‘/content/drive/‘) change dirimport osos.chdir(“/content/drive/My Drive/Colab Notebooks/…/“) Do not use same browser to run both Jupyter and Colab, might crash Take Home Project 2 - sentiment classifierpreperation good examples and guide: bentrevett / pytorch-sentiment-analysis / Sentiment Analysis with Pytorch — Part 3— CNN Model / xalanq/ chinese-sentiment-classification / slaysd/ pytorch-sentiment-analysis-classification NN, RNN, BERT - given Jupyter files modify CNN Jupyter files change surname model to sentiment model - character level tokens online reference: 3 kind of outcome sentiment classifer use weka:weka to do deep learning / official guide of installing / how to quick install weka deep learning toolkit Further work direction coBerta kaggle example using coBerta weka preprocessing + bert weka preprocessing evaluation ambert evaluation / ambert discussion in zhihu Group Project : Taylor Swift lyrics generatormethod: RNN LSTM GPT2 LSTM github good instance LSTM github detailed instance - Tom-Chang-Deep-Lyrics | 基於 LSTM 深度學習方法研發而成的張雨生歌詞產生模型，致敬張雨生。 GPT2 styled lyrics generator - github + colab Lab-no2 adjust hyperparameters to get higher accuracy and lower lossOptimize target: Classifying Surnames with a Multilayer Perceptronoriginal code from the textbook website, https://github.com/joosthub/PyTorchNLPBook. Build the best model (based on test loss and test accuracy) by exploring following options: learning_rate batch_size dropout (use only if it helps) batch norm (use only if it helps) weight_decay (L2 regularization) (use only if it helps) hidden_dim Note that it is not necessary to adjust other parameter values even though you are allowed to do so. Values of given parametersThanks for Professor Jin’s suggestion and instruction, I would choose to share some findings when using a slightly different input dataset later on, but not share my answer directly. Best outcomesTest loss: 1.61;Test Accuracy: 57.789 Taking “drewer” as an exampleTop 15 predictions: drewer -&gt; German (p=0.46)drewer -&gt; English (p=0.35)drewer -&gt; Dutch (p=0.06)drewer -&gt; Scottish (p=0.04)drewer -&gt; Czech (p=0.04)drewer -&gt; Polish (p=0.01)drewer -&gt; Spanish (p=0.01)drewer -&gt; French (p=0.01)drewer -&gt; Portuguese (p=0.01)drewer -&gt; Russian (p=0.00)drewer -&gt; Irish (p=0.00)drewer -&gt; Chinese (p=0.00)drewer -&gt; Japanese (p=0.00)drewer -&gt; Italian (p=0.00)drewer -&gt; Arabic (p=0.00) Tips Using VPN would block anaconda-navigator startup, because VPN would probably take up the specific localhost port. homebrew to solve environment path problem. Install graphviz 12brew install graphvizpip install graphviz","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Text & Web Mining","slug":"Courses/Text-Web-Mining","permalink":"http://muyuhuatang.github.io/categories/Courses/Text-Web-Mining/"}],"tags":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"}]},{"title":"Encrypted","slug":"Encrypted","date":"2020-09-24T14:44:45.544Z","updated":"2020-09-25T12:57:58.249Z","comments":true,"path":"2020/09/24/Encrypted/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/24/Encrypted/","excerpt":"Here's something encrypted, password is required to continue reading. Feel free to email me to get the password with fair reason.","text":"Hey, password is required here.77d7535348f6230f63550361cfb8db528ba5fc8bf8a16bd484ce4555d23e1db498f218e407f5f4fd1376813172648eed36c93ab6e576283afeb28b60b802620d10f43f3012706b714f7182fc1ac4714035172207b318c24edd9640b7d68e53cfefaac49c385b05691dbe62d4c01afdd8ee8fd23f35245c11c9612e6977448f6410e1eeeb7cd9e8fa40118efca5fd2c6719355afb101b12770dcd247a02365ef5","categories":[{"name":"Documents","slug":"Documents","permalink":"http://muyuhuatang.github.io/categories/Documents/"}],"tags":[{"name":"Encrypted","slug":"Encrypted","permalink":"http://muyuhuatang.github.io/tags/Encrypted/"}]},{"title":"Information Visualization","slug":"Information-Visualization","date":"2020-09-20T13:57:02.333Z","updated":"2020-11-13T04:16:34.568Z","comments":true,"path":"2020/09/20/Information-Visualization/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/20/Information-Visualization/","excerpt":"Information Visualization course","text":"Information Visualization course Storytelling about Covid-19 - LIFE of LIPreparation Very good D3 example and implementation website - observablehq.com Base html with swig special effect Good examples: small man animation effect functioned by scrolling from up to down / 3D special effect with zoom function / The New York Times - football races - information dashboard Information Visualization dashboard of GE2020Preparation analysis about previews election date: about election time and timespan - sg_elections_cch Singapore Official election data js onine small tester - w3schools.com color code in javascript MapBox Official websit / Official demo / [official ] Election Boundarys from Singapore offcial database / Small tool KMZ to GeoJson convertor Add a GeoJSON polygon - mapbox How to use mapbox to implement the function [mouse move in show popup and mouse move out unshow popup] meterial: Official document / Implementation A excellent online implementation / Official website of ZaoBao GE2020 page further explore: mapbox-gl github guide of installation / implementation about mapbox-gl - Blog / an interesting github program not successfully deployed / mapbox and python analysis - Blog might useful: add a pattern to a polygon D3 script to geojson How to fix the problem that can not read loal files when testing in browser firefox error report How to set Sublime Text to auto change lines in the view when one single line is too long The much too high density of edge data could possible cause D3 crash and output a BIG SQUARE on the screen. So if this happens, try to use lower presion data. (Lower means much fewer coordinate point pairs). While on the other hand, this crash would not happen on Mapbox. It not vary hard to find out and understand. For example, CHOA CHU KANG of singapore in the official ge2020 boundary data. But at the same time, the EAST COAST is also very huge in the amount of coordinate pairs and it does not make D3 crash. So maybe the true reason still remain exploration. If you find something, I would be very pleasure if you share your discovery via email(&#x66;&#104;&#117;&#x61;&#110;&#x67;&#49;&#56;&#x31;&#x40;&#x67;&#109;&#97;&#105;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;). :)———— After the test in the WEST COAST data, it could be sure that the problem lies in the to much detialed borders of districts data provided by goveronment, which is very hard for d3 to recongnize. There is a slow but very useful way to handle this problem. I can just try to delete randomly some coordinate pairs and test if the output is valid. Again and again, the iteration of this process could finally focus on the real “bad points”. [Random select - Top2Down approximation - Down2Top approximaiton] is my approch. This problem could also be fixed by changing data density of the geojson file. How to draw and control d3 colors Use website to modify geomap.json file Merge projects The html would possible automatically generate a opacity style with random value, it could be a headache. put the line in the body title would kindly fix this problem 1&lt;body style&#x3D;&quot;&#x2F;*! opacity:1; *&#x2F;&quot;&gt; how to merge a html file with v3 and v4 of d3 framework: download d3.v3 to local files and use d3.v3 as a local javascript. You can see the project through this linkD3 usage notes D3Js.v5: …selectAll(…).data(…).enter is not a function ERROR example: 03_D3_TSV_Loading.html codes:12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;D3: Loading data from a TSV file&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;d3.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;script type=&quot;text/javascript&quot;&gt; d3.tsv(&quot;employees.tsv&quot;).then(function(data) &#123; for (var i = 0; i &lt; data.length; i++)&#123; console.log(data[i].Name); console.log(data[i].Age); &#125;&#125;); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; InfographicMy classmates’ excellent works:I have get the permission from my classmates, and I sort works by alphabetic order of their names. HUANG TIANTIAN - dream_space HE XINYI - infographic_hxy MA GUANGYI - why_are_cars_so_expensive_in_singaporeDribbble Link PETER LING CHONG TECK - cybercrime My self-made infographic and its report. If you have any suggestions or ideas, I am very glad to recieve your letter. Feel welcomed to email me. :) My email: &#x66;&#104;&#117;&#97;&#110;&#x67;&#49;&#x38;&#x31;&#x40;&#x67;&#x6d;&#97;&#105;&#x6c;&#x2e;&#99;&#x6f;&#109;","categories":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Information Visualization","slug":"Courses/Information-Visualization","permalink":"http://muyuhuatang.github.io/categories/Courses/Information-Visualization/"}],"tags":[{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"}]},{"title":"Book Notes - Chinese","slug":"Book-notes","date":"2020-09-18T16:17:52.416Z","updated":"2020-12-15T11:39:09.761Z","comments":true,"path":"2020/09/19/Book-notes/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/19/Book-notes/","excerpt":"Here's something encrypted, password is required to continue reading. Feel free to email me to get the password with fair reason.","text":"Hey, password is required here.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","categories":[{"name":"BookNotes","slug":"BookNotes","permalink":"http://muyuhuatang.github.io/categories/BookNotes/"}],"tags":[{"name":"Records","slug":"Records","permalink":"http://muyuhuatang.github.io/tags/Records/"},{"name":"BookNotes","slug":"BookNotes","permalink":"http://muyuhuatang.github.io/tags/BookNotes/"},{"name":"Encrypted","slug":"Encrypted","permalink":"http://muyuhuatang.github.io/tags/Encrypted/"}]},{"title":"Python Learning","slug":"Python-Learning","date":"2020-09-08T09:47:33.205Z","updated":"2020-09-25T12:58:23.626Z","comments":true,"path":"2020/09/08/Python-Learning/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/08/Python-Learning/","excerpt":"The introduciton of Python and some notes when learning","text":"The introduciton of Python and some notes when learning Notes for 《Learn Python The Hard Way》: Use PEMDAS(Parentheses Exponents Multiplication Division Addition Subtraction) to help remeber the calculating priority. Could use “%r” to debug because it will show the raw data of the parameter. Command Line programming is very important for beginners. Reference: 《Learn Python The Hard Way》 This book is suitable for person who did not know much about Computer Programming and Python before. This book indicate some good habits programer should posses, which I believe is very good for beginners. Reading time apporximately is","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"http://muyuhuatang.github.io/categories/Programming-Language/"}],"tags":[{"name":"Notes","slug":"Notes","permalink":"http://muyuhuatang.github.io/tags/Notes/"},{"name":"Python","slug":"Python","permalink":"http://muyuhuatang.github.io/tags/Python/"}]},{"title":"Blog start-up and settings","slug":"blog","date":"2020-09-03T17:50:15.401Z","updated":"2020-10-23T03:10:12.376Z","comments":true,"path":"2020/09/04/blog/","link":"","permalink":"http://muyuhuatang.github.io/2020/09/04/blog/","excerpt":"Welcome to Mysite!","text":"Welcome to Mysite! Hexo Official ReferenceHexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Hexo TipsCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment References:Building: How to build this blog and some small improvements How to deploy Articles How to optimize Visualization Performance of Next - 1 - 2 Tips: only for reference, many method in these two pages are outdate. Adjusting: How to activate “tags” and “categories” functionsStep 1Step 2 How to enable the search function in blog How to add “top” function Tips: 1) the value of &quot;top&quot; tag bigger, the rank of article higher Word count and read time function Visiting number count function Tips: 1) This is an embedded plugin in NexT theme 2) the homepage of [&quot;busuanzi&quot;](http://ibruce.info/2015/04/04/busuanzi/) Use Taobao npm to accelerate installation process How to avoid form error in hexo display Modifying/Updating: Use hexo-hey to manage Blog hexo-hey github source code page How to store the images in github repository 2.1. (Recommand)Use plug-in unit of relative routine / Embedding an image using markdown 2.1.1. Plug-in unit format: put the image in “source/_post/[your new article’s tile]/“ 1&#123;% asset_img [imageFullName] [imamge descirption]] %&#125; 2.1.2. Embedding an image using markdown: do not have to put the in the specific “public” file because the image there would be automatically generated if you have put the image in the “source/_post/[your new article’s tile]/“ 1![[descirption]]([imageFullName]) 2.2. (Sometimes would fail to load)Use hexo-hey funtion Put the image in the “source/images/“: I changed 236th row in file “node_modules/hexo-hey/api.js” with this step and adjust 1filename: hexo.config.url + &#x27;/&#x27; + req.file.filename to 1filename: &#x27;/&#x27; + req.file.filename So, I could use the format below: 1![[descirption]](/images/[imageFullName]) How to preview pdf files in Blog Github: Hexo-pdf Use object tag &amp; asset_floder to load pdf 12&lt;object data=&quot;./[pdfFullName]&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;800px&quot;&gt;&lt;/object&gt;&lt;object data=&quot;[previewURL]&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;800px&quot;&gt;&lt;/object&gt; Tips: 1) set height as specific px but not percentage. 2) use local reference first, because URL need much more time to load. 3) if you want to insert a pdf file in &quot;about&quot; page, you should put the file directly in &quot;source/about/&quot; How to use encrypt function for specific articles Tips: 1) download of hexo-blog-encrypt may fail using npm method, try yarn method instead. 2) TOC function may disable encrypted article loading, so you can set toc as &#39;false&#39; in config file of theme. How to add canvas special effects Canvas-ribbon Canvas-three How to recommand related articles at the bottom How to out Jupyter notebook in hexo————unstable and not available now. Should ues “download the file in jupyter as MD file” + “ hide article in hexo net” How to show independent website page in the Github Pages blogs restore all the needed website page in the /myDoc file when need to show through Github Pages, paste the file directly in /public file. Generate and deploy then would be fine to show throung relative path in the blog. Remember! the [hexo clean] command would erase [myDoc] file, so it is needed to restore the file in the root path of Blog","categories":[{"name":"Documents","slug":"Documents","permalink":"http://muyuhuatang.github.io/categories/Documents/"}],"tags":[{"name":"Records","slug":"Records","permalink":"http://muyuhuatang.github.io/tags/Records/"}]}],"categories":[{"name":"Records","slug":"Records","permalink":"http://muyuhuatang.github.io/categories/Records/"},{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/categories/Courses/"},{"name":"Data Mining","slug":"Courses/Data-Mining","permalink":"http://muyuhuatang.github.io/categories/Courses/Data-Mining/"},{"name":"Juypter","slug":"Juypter","permalink":"http://muyuhuatang.github.io/categories/Juypter/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"http://muyuhuatang.github.io/categories/Data-Structure/"},{"name":"LeetCode","slug":"Data-Structure/LeetCode","permalink":"http://muyuhuatang.github.io/categories/Data-Structure/LeetCode/"},{"name":"Data Science","slug":"Data-Science","permalink":"http://muyuhuatang.github.io/categories/Data-Science/"},{"name":"Documents","slug":"Documents","permalink":"http://muyuhuatang.github.io/categories/Documents/"},{"name":"Intrusion Detection","slug":"Courses/Intrusion-Detection","permalink":"http://muyuhuatang.github.io/categories/Courses/Intrusion-Detection/"},{"name":"Internet Programming","slug":"Courses/Internet-Programming","permalink":"http://muyuhuatang.github.io/categories/Courses/Internet-Programming/"},{"name":"Text & Web Mining","slug":"Courses/Text-Web-Mining","permalink":"http://muyuhuatang.github.io/categories/Courses/Text-Web-Mining/"},{"name":"Information Visualization","slug":"Courses/Information-Visualization","permalink":"http://muyuhuatang.github.io/categories/Courses/Information-Visualization/"},{"name":"BookNotes","slug":"BookNotes","permalink":"http://muyuhuatang.github.io/categories/BookNotes/"},{"name":"Programming Language","slug":"Programming-Language","permalink":"http://muyuhuatang.github.io/categories/Programming-Language/"}],"tags":[{"name":"Tools","slug":"Tools","permalink":"http://muyuhuatang.github.io/tags/Tools/"},{"name":"Data Mining","slug":"Data-Mining","permalink":"http://muyuhuatang.github.io/tags/Data-Mining/"},{"name":"Interesting","slug":"Interesting","permalink":"http://muyuhuatang.github.io/tags/Interesting/"},{"name":"Record","slug":"Record","permalink":"http://muyuhuatang.github.io/tags/Record/"},{"name":"Courses","slug":"Courses","permalink":"http://muyuhuatang.github.io/tags/Courses/"},{"name":"Encrypted","slug":"Encrypted","permalink":"http://muyuhuatang.github.io/tags/Encrypted/"},{"name":"Records","slug":"Records","permalink":"http://muyuhuatang.github.io/tags/Records/"},{"name":"BookNotes","slug":"BookNotes","permalink":"http://muyuhuatang.github.io/tags/BookNotes/"},{"name":"Notes","slug":"Notes","permalink":"http://muyuhuatang.github.io/tags/Notes/"},{"name":"Python","slug":"Python","permalink":"http://muyuhuatang.github.io/tags/Python/"}]}